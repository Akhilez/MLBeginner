{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AkkiTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhilez/justcode/blob/master/Assignments/DL/hw8/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Lzcs3mewEd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Deep Learning CS6073 Assignment 8\n",
        "\n",
        "    By Akhil Kanna Devarashetti\n",
        "    \n",
        "Question:\n",
        "\n",
        "    This programming assignment is based on https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "    But we will only run the Transformer.\n",
        "    Download train.txt, valid.txt, and test.txt to ./data/wikitext-2/.\n",
        "    You may need to run python main.py with specification of the selection of Transformer, or python main2.py, \n",
        "    which along with model2.py, is a simplified version only for the Transformer and with a few epochs.\n",
        "    We need data.py to start\n",
        "    and generate.py to show the learning result.\n",
        "    Show that you indeed have spent time in studying and running the programs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q19y8dI2aY-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from io import open\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uktV4F5xTL0H",
        "colab_type": "code",
        "outputId": "56e3fc8f-8915-4097-a98f-b9b73b621745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!apt-get install tree"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 1s (63.6 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 145118 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuwsKeh4e_Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bptt = 20\n",
        "loginterval = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ0XwFDDffot",
        "colab_type": "code",
        "outputId": "592f3098-dca7-43ad-c92f-4e761b9fd329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!mkdir -p data/wikitext-2\n",
        "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt > data/wikitext-2/test.txt\n",
        "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt > data/wikitext-2/train.txt\n",
        "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt > data/wikitext-2/valid.txt\n",
        "!ls data/wikitext-2/"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1227k  100 1227k    0     0  4445k      0 --:--:-- --:--:-- --:--:-- 4445k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10.2M  100 10.2M    0     0  27.2M      0 --:--:-- --:--:-- --:--:-- 27.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1095k  100 1095k    0     0  4416k      0 --:--:-- --:--:-- --:--:-- 4416k\n",
            "test.txt  train.txt  valid.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIrSQgI0H109",
        "colab_type": "text"
      },
      "source": [
        "## Lets take a small dataset to analyse what's happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uualMPRYAy5U",
        "colab_type": "code",
        "outputId": "7d204566-50da-4dbc-fb3c-1eae8fc729d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!mkdir -p data/wikitext-2\n",
        "!echo -e 'a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox' > data/wikitext-2/test.txt\n",
        "!echo -e \"the quick brown fox moves faster than the lazy dog\\nthis is because the quick brown fox is smarter than the lazy dog\" > data/wikitext-2/train.txt\n",
        "!echo -e \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox\" > data/wikitext-2/valid.txt\n",
        "!tree data"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\n",
            "└── wikitext-2\n",
            "    ├── test.txt\n",
            "    ├── train.txt\n",
            "    └── valid.txt\n",
            "\n",
            "1 directory, 3 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqQq0ZKhYLkn",
        "colab_type": "code",
        "outputId": "6b04fb80-c781-40eb-c0e3-8b78bdc809fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!cat data/wikitext-2/train.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the quick brown fox moves faster than the lazy dog\n",
            "this is because the quick brown fox is smarter than the lazy dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV7nXFvNIaJJ",
        "colab_type": "text"
      },
      "source": [
        "## The dictionary class saves the mapping between the words and their indices.\n",
        "## Every word is assigned an integer based on the order of its occurance.\n",
        "\n",
        "    Example input: \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox <eos>\"\n",
        "\n",
        "    Output: \n",
        "    {'a': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'the': 6, 'lazy': 7, 'dog': 8, 'is': 9, 'not': 10, '<eos>': 11}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnN3pZAWIZKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFigxgkpRB-1",
        "colab_type": "code",
        "outputId": "5fd054ea-e8fa-4663-eb59-1485179257ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_input = \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox <eos>\"\n",
        "\n",
        "sample_dict = Dictionary()\n",
        "\n",
        "for word in sample_input.split():\n",
        "    sample_dict.add_word(word)\n",
        "\n",
        "print(sample_dict.word2idx)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'the': 6, 'lazy': 7, 'dog': 8, 'is': 9, 'not': 10, '<eos>': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i09vm9cSVu9",
        "colab_type": "text"
      },
      "source": [
        "Corpus class will create the dictionary for all three datasets.\n",
        "\n",
        "    For example:\n",
        "    test data = \"a quick brown fox jumps over the lazy dog a lazy dog is not a brown fox\"\n",
        "    corpus.test = tensor([14,  1,  2,  3, 15, 16,  0,  7, 17,  7, 12,  9, 18, 14,  2,  3, 13])\n",
        "\n",
        "Note that 'a' is repeated twice and we see it's id 14 repeated twice too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ksgpgsifz1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqJUr8gig0Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "\n",
        "corpus = Corpus('./data/wikitext-2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEHBU_Q9XLxb",
        "colab_type": "code",
        "outputId": "f1a3af20-887e-40b9-accd-4d3168a14f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(f'Dictionary: {corpus.dictionary.word2idx}')\n",
        "print('\\nTest data:')\n",
        "!cat data/wikitext-2/test.txt\n",
        "print(f'\\ncorpus.test: {corpus.test}')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary: {'the': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'moves': 4, 'faster': 5, 'than': 6, 'lazy': 7, 'dog': 8, '<eos>': 9, 'this': 10, 'is': 11, 'because': 12, 'smarter': 13, 'a': 14, 'jumps': 15, 'over': 16, 'not': 17}\n",
            "\n",
            "Test data:\n",
            "a quick brown fox jumps over the lazy dog\n",
            "a lazy dog is not a brown fox\n",
            "\n",
            "corpus.test: tensor([14,  1,  2,  3, 15, 16,  0,  7,  8,  9, 14,  7,  8, 11, 17, 14,  2,  3,\n",
            "         9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbOPYIxuQnVF",
        "colab_type": "text"
      },
      "source": [
        "As given in the documentation, the batchify function will take the complete dataset as input string and converts it into batches.\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFt9nrGgQYp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbnyLS9tVGHi",
        "colab_type": "code",
        "outputId": "7cd4dace-c90d-417b-e311-c42624fa1586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(corpus.test)\n",
        "sample_batches = batchify(corpus.test, 3)\n",
        "print(sample_batches)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   0,    1, 1144,  ...,   15,    0,    0])\n",
            "tensor([[    0,   151,    17],\n",
            "        [    1,    17,     9],\n",
            "        [ 1144,  2533,    37],\n",
            "        ...,\n",
            "        [   46,   912,     9],\n",
            "        [  131, 17741,    15],\n",
            "        [  997,    61,     0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_0DTcZUvm1i",
        "colab_type": "text"
      },
      "source": [
        "get_batch will actually create a batch of sequence size bptt from index i. Source is the output from batchify()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4A68NRSQfIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i + seq_len]\n",
        "    target = source[i + 1:i + 1 + seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ZwPYXSa4KI",
        "colab_type": "code",
        "outputId": "86b4c942-1ea3-42d9-dd8c-e493d7eacdb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(get_batch(sample_batches, 0))\n",
        "print(get_batch(sample_batches, 1))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[14,  0,  8],\n",
            "        [ 1,  7, 11],\n",
            "        [ 2,  8, 17],\n",
            "        [ 3,  9, 14],\n",
            "        [15, 14,  2]]), tensor([ 1,  7, 11,  2,  8, 17,  3,  9, 14, 15, 14,  2, 16,  7,  3]))\n",
            "(tensor([[ 1,  7, 11],\n",
            "        [ 2,  8, 17],\n",
            "        [ 3,  9, 14],\n",
            "        [15, 14,  2]]), tensor([ 2,  8, 17,  3,  9, 14, 15, 14,  2, 16,  7,  3]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktYWnHTBQVjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gg0WElg70T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        print(f'after mult with embedding = {src}')\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhhalf99hLr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emsize = 100\n",
        "nhead = 2\n",
        "nhid = 64\n",
        "nlayers = 2\n",
        "dropout = 0.2\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2GxEh8ZhU-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        model.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % loginterval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / loginterval\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPI3C9qdz-_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    print(data_source.shape)\n",
        "    return total_loss / (len(data_source))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma2Je00AhW_z",
        "colab_type": "code",
        "outputId": "5e368534-9f29-430b-963b-dddcc448c83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 5\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open('model.pt', 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  0.00 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "torch.Size([1, 10])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  0.00 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "torch.Size([1, 10])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  0.00 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "torch.Size([1, 10])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  0.00 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  0.00 | valid ppl     1.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKXJ5WhNhZM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}