{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_with_comments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K3Lzcs3mewEd"
      },
      "source": [
        "\n",
        "# Deep Learning CS6073 Assignment 8\n",
        "\n",
        "    By Akhil Kanna Devarashetti\n",
        "    \n",
        "### Question:\n",
        "\n",
        "    This programming assignment is based on https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "    But we will only run the Transformer.\n",
        "    Download train.txt, valid.txt, and test.txt to ./data/wikitext-2/.\n",
        "    You may need to run python main.py with specification of the selection of Transformer, or python main2.py, \n",
        "    which along with model2.py, is a simplified version only for the Transformer and with a few epochs.\n",
        "    We need data.py to start\n",
        "    and generate.py to show the learning result.\n",
        "    Show that you indeed have spent time in studying and running the programs.\n",
        "    \n",
        "_____\n",
        " \n",
        " To show that I studied the program and transformer model, I'm going to add comments to each part of the program.\n",
        " I divided the complete program into separate parts here so that I can describe what is happening at each step.\n",
        " \n",
        " The whole point of this code is to predict the next word given a sequence.\n",
        " Transformer model can do it quite well without using recurrent architecture.\n",
        " \n",
        " In this notebook, I am executing a model with a very small dataset to check how things work.\n",
        " But, the execution of the model with the complete dataset is present in the *other notebook* titled `full_execution.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q19y8dI2aY-J",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from io import open\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PuwsKeh4e_Wv",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "bptt = 4 # Reduced from 20 to check how it works.\n",
        "loginterval = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ie_hWHcE9Wu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d1c00cef-f61a-495b-fec1-1af74188efbe"
      },
      "source": [
        "!apt-get install tree  # If tree doesn't work"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tree is already the newest version (1.7.0-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13CBNUrCEZCi",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the dataset into `data/wikitext-2` using bash script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hJ0XwFDDffot",
        "outputId": "87e8c4a8-4799-4f05-9dd7-009dda65060e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!mkdir -p data/wikitext-2\n",
        "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt > data/wikitext-2/test.txt\n",
        "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt > data/wikitext-2/train.txt\n",
        "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt > data/wikitext-2/valid.txt\n",
        "!ls data/wikitext-2/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1227k  100 1227k    0     0  27.2M      0 --:--:-- --:--:-- --:--:-- 27.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10.2M  100 10.2M    0     0   117M      0 --:--:-- --:--:-- --:--:--  117M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1095k  100 1095k    0     0  27.4M      0 --:--:-- --:--:-- --:--:-- 27.4M\n",
            "test.txt  train.txt  valid.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IIrSQgI0H109"
      },
      "source": [
        "## Lets take a small dataset to analyse what's happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uualMPRYAy5U",
        "outputId": "0f3d5c5f-f2b3-485f-b76d-630f2f275beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!mkdir -p data/sample/\n",
        "!echo -e 'a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox' > data/sample/test.txt\n",
        "!echo -e \"the quick brown fox moves faster than the lazy dog\\nthis is because the quick brown fox is smarter than the lazy dog\" > data/sample/train.txt\n",
        "!echo -e \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox\" > data/sample/valid.txt\n",
        "# !apt-get install tree  # If tree doesn't work\n",
        "!tree data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\n",
            "├── sample\n",
            "│   ├── test.txt\n",
            "│   ├── train.txt\n",
            "│   └── valid.txt\n",
            "└── wikitext-2\n",
            "    ├── test.txt\n",
            "    ├── train.txt\n",
            "    └── valid.txt\n",
            "\n",
            "2 directories, 6 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iqQq0ZKhYLkn",
        "outputId": "be0b4c09-5da3-4910-a5c9-eac96e54ec65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!cat data/sample/train.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the quick brown fox moves faster than the lazy dog\n",
            "this is because the quick brown fox is smarter than the lazy dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kV7nXFvNIaJJ"
      },
      "source": [
        "## The dictionary class saves the mapping between the words and their indices.\n",
        "## Every word is assigned an integer based on the order of its occurance.\n",
        "\n",
        "    Example input: \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox <eos>\"\n",
        "\n",
        "    Output: \n",
        "    {'a': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'the': 6, 'lazy': 7, 'dog': 8, 'is': 9, 'not': 10, '<eos>': 11}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hnN3pZAWIZKr",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KFigxgkpRB-1",
        "outputId": "ab9df396-9f65-4382-cf0c-981d91a32424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_input = \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox <eos>\"\n",
        "\n",
        "sample_dict = Dictionary()\n",
        "\n",
        "for word in sample_input.split():\n",
        "    sample_dict.add_word(word)\n",
        "\n",
        "print(sample_dict.word2idx)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'the': 6, 'lazy': 7, 'dog': 8, 'is': 9, 'not': 10, '<eos>': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-i09vm9cSVu9"
      },
      "source": [
        "### Corpus class will create the dictionary for all three datasets.\n",
        "\n",
        "    For example:\n",
        "    test data = \"a quick brown fox jumps over the lazy dog a lazy dog is not a brown fox\"\n",
        "    corpus.test = tensor([14,  1,  2,  3, 15, 16,  0,  7, 17,  7, 12,  9, 18, 14,  2,  3, 13])\n",
        "\n",
        "Note that 'a' is repeated twice and we see it's id 14 repeated twice too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ksgpgsifz1j",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qqJUr8gig0Dc",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "batch_size = 4 # 20\n",
        "eval_batch_size = 3 # 10\n",
        "\n",
        "#corpus = Corpus('./data/wikitext-2')\n",
        "corpus = Corpus('./data/sample')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEHBU_Q9XLxb",
        "outputId": "0734ad16-2f1e-46cd-9f17-e8f055aaab1f",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(f'Dictionary: {corpus.dictionary.word2idx}')\n",
        "print('\\nTest data:')\n",
        "!cat data/sample/test.txt\n",
        "print(f'\\ncorpus.test: {corpus.test}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary: {'the': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'moves': 4, 'faster': 5, 'than': 6, 'lazy': 7, 'dog': 8, '<eos>': 9, 'this': 10, 'is': 11, 'because': 12, 'smarter': 13, 'a': 14, 'jumps': 15, 'over': 16, 'not': 17}\n",
            "\n",
            "Test data:\n",
            "a quick brown fox jumps over the lazy dog\n",
            "a lazy dog is not a brown fox\n",
            "\n",
            "corpus.test: tensor([14,  1,  2,  3, 15, 16,  0,  7,  8,  9, 14,  7,  8, 11, 17, 14,  2,  3,\n",
            "         9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DbOPYIxuQnVF"
      },
      "source": [
        "### As given in the documentation, the batchify function will take the complete dataset as input string and converts it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pFt9nrGgQYp9",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HbnyLS9tVGHi",
        "outputId": "be0adea8-9d72-4e73-9d7d-3e00144e428c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(corpus.test)\n",
        "sample_batches = batchify(corpus.test, 3)\n",
        "print(sample_batches)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([14,  1,  2,  3, 15, 16,  0,  7,  8,  9, 14,  7,  8, 11, 17, 14,  2,  3,\n",
            "         9])\n",
            "tensor([[14,  0,  8],\n",
            "        [ 1,  7, 11],\n",
            "        [ 2,  8, 17],\n",
            "        [ 3,  9, 14],\n",
            "        [15, 14,  2],\n",
            "        [16,  7,  3]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_0DTcZUvm1i"
      },
      "source": [
        "### get_batch will actually create a batch of sequence size bptt from index i. Source is the output from batchify()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A4A68NRSQfIP",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i + seq_len]\n",
        "    target = source[i + 1:i + 1 + seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B3ZwPYXSa4KI",
        "outputId": "dfe56786-1fe9-48e1-d2b9-db5e9fecadc2",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(get_batch(sample_batches, 0))\n",
        "print(get_batch(sample_batches, 1))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[14,  0,  8],\n",
            "        [ 1,  7, 11],\n",
            "        [ 2,  8, 17],\n",
            "        [ 3,  9, 14]]), tensor([ 1,  7, 11,  2,  8, 17,  3,  9, 14, 15, 14,  2]))\n",
            "(tensor([[ 1,  7, 11],\n",
            "        [ 2,  8, 17],\n",
            "        [ 3,  9, 14],\n",
            "        [15, 14,  2]]), tensor([ 2,  8, 17,  3,  9, 14, 15, 14,  2, 16,  7,  3]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktYWnHTBQVjU",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p9gg0WElg70T",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c01fc3af-f25c-4583-82f9-4c5d51dbb74e"
      },
      "source": [
        "print(f'\\nword_idx: \\n{corpus.dictionary.word2idx}')\n",
        "print(f'\\nTrain data: {train_data.shape}\\n{train_data}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "word_idx: \n",
            "{'the': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'moves': 4, 'faster': 5, 'than': 6, 'lazy': 7, 'dog': 8, '<eos>': 9, 'this': 10, 'is': 11, 'because': 12, 'smarter': 13, 'a': 14, 'jumps': 15, 'over': 16, 'not': 17}\n",
            "\n",
            "Train data: torch.Size([6, 4])\n",
            "tensor([[ 0,  6, 11, 11],\n",
            "        [ 1,  0, 12, 13],\n",
            "        [ 2,  7,  0,  6],\n",
            "        [ 3,  8,  1,  0],\n",
            "        [ 4,  9,  2,  7],\n",
            "        [ 5, 10,  3,  8]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbV5wqKLEZDY",
        "colab_type": "text"
      },
      "source": [
        "### The cell below is the implementation of Positional Encoder in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "d9C3GXc7EZDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "GiwkI99BEZDb",
        "colab_type": "text"
      },
      "source": [
        "### The cell below is the implementation of Transformer network. It consists of the following flow for a *SINGLE* batch:\n",
        "\n",
        "## 1. Input:\n",
        "Each sample in a batch is an array of size S where each element is an integer which is an ID of a word in the vocabulary.\n",
        "\n",
        "Input shape: (S)  (S = sequence length = bptt = 20)\n",
        "\n",
        "Example: [1, 45, 3, ...]\n",
        "\n",
        "## 2. Embedding:\n",
        "The embedding layer generates a real number array of size E for each integer (word).\n",
        "\n",
        "Output shape: (S, E)   (E = Embedding size) (E = 100)\n",
        "\n",
        "Example input: [(1.2, 0.3, 4.5), (x, x, x), ... (x, x, x)] For E = 3\n",
        "\n",
        "## 3. Positional Encoding:\n",
        "Positional encoding will encode the relative positions of the words in the sequence.\n",
        "\n",
        "Output shape: (S, E)\n",
        "\n",
        "Example input: [(1.2, 0.3, 4.5), (x, x, x), ... (x, x, x)] For E = 3\n",
        "\n",
        "## 4. Transformer Encoding:\n",
        "This module does the rest of the transformer professes with keys, values etc internally.\n",
        "\n",
        "Output shape: (S, E)\n",
        "\n",
        "Example input: [(1.2, 0.3, 4.5), (x, x, x), ... (x, x, x)] For E = 3\n",
        "\n",
        "## 5. Decoder:\n",
        "This is a fully-connected layer that outputs a vector size of vocabulary V for each embedding.\n",
        "\n",
        "Output shape: (S, V)  (V = Vocabulary size)\n",
        "\n",
        "Example input: [(1.2, 0.3), (x, x), ... (x, x)] For V = 2\n",
        "\n",
        "## 6. Softmax:\n",
        "This module converts each of the V size arrays to probability distributions. The word with the highest probability will be the next word.\n",
        "\n",
        "Output shape: (S, V)  \n",
        "\n",
        "Example input: [(0.7, 0.3), (x, x), ... (x, x)] For V = 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_fpXooK5EZDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        print(f'\\nSource: {src.shape}')\n",
        "        src = self.encoder(src)\n",
        "        print(f'after embedding = {src.shape}')\n",
        "        src = src * math.sqrt(self.ninp)\n",
        "        print(f'Multiplier = {math.sqrt(self.ninp)}')\n",
        "        print(f'after mult with embedding = {src.shape}')\n",
        "        src = self.pos_encoder(src)\n",
        "        print(f'after positional enc = {src.shape}')\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        print(f'after transformer enc = {output.shape}')\n",
        "        output = self.decoder(output)\n",
        "        print(f'after decoder = {output.shape}')\n",
        "        softmax_output = F.log_softmax(output, dim=-1)\n",
        "        print(f'after softmax = {softmax_output.shape}')\n",
        "        return softmax_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zM2Em4-EZDg",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the values for the architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zhhalf99hLr5",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "emsize = 100  # E = Embedding size\n",
        "nhead = 2     # Number of heads in the transformer\n",
        "nhid = 64     # Hidden units size in the transformer\n",
        "nlayers = 2\n",
        "dropout = 0.2\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUL4vSTXEZDj",
        "colab_type": "text"
      },
      "source": [
        "### Defining the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n2GxEh8ZhU-x",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        model.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % loginterval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / loginterval\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XPI3C9qdz-_1",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    print(data_source.shape)\n",
        "    return total_loss / (len(data_source))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ma2Je00AhW_z",
        "outputId": "695b680a-d2a8-4c76-b9f7-6cad23ff12c1",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 5\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open('model.pt', 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Source: torch.Size([4, 4])\n",
            "after embedding = torch.Size([4, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 4, 100])\n",
            "after positional enc = torch.Size([4, 4, 100])\n",
            "after transformer enc = torch.Size([4, 4, 100])\n",
            "after decoder = torch.Size([4, 4, 18])\n",
            "after softmax = torch.Size([4, 4, 18])\n",
            "\n",
            "Source: torch.Size([1, 4])\n",
            "after embedding = torch.Size([1, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 4, 100])\n",
            "after positional enc = torch.Size([1, 4, 100])\n",
            "after transformer enc = torch.Size([1, 4, 100])\n",
            "after decoder = torch.Size([1, 4, 18])\n",
            "after softmax = torch.Size([1, 4, 18])\n",
            "\n",
            "Source: torch.Size([4, 3])\n",
            "after embedding = torch.Size([4, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 3, 100])\n",
            "after positional enc = torch.Size([4, 3, 100])\n",
            "after transformer enc = torch.Size([4, 3, 100])\n",
            "after decoder = torch.Size([4, 3, 18])\n",
            "after softmax = torch.Size([4, 3, 18])\n",
            "\n",
            "Source: torch.Size([1, 3])\n",
            "after embedding = torch.Size([1, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 3, 100])\n",
            "after positional enc = torch.Size([1, 3, 100])\n",
            "after transformer enc = torch.Size([1, 3, 100])\n",
            "after decoder = torch.Size([1, 3, 18])\n",
            "after softmax = torch.Size([1, 3, 18])\n",
            "torch.Size([6, 3])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  2.52 | valid ppl    12.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "Source: torch.Size([4, 4])\n",
            "after embedding = torch.Size([4, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 4, 100])\n",
            "after positional enc = torch.Size([4, 4, 100])\n",
            "after transformer enc = torch.Size([4, 4, 100])\n",
            "after decoder = torch.Size([4, 4, 18])\n",
            "after softmax = torch.Size([4, 4, 18])\n",
            "\n",
            "Source: torch.Size([1, 4])\n",
            "after embedding = torch.Size([1, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 4, 100])\n",
            "after positional enc = torch.Size([1, 4, 100])\n",
            "after transformer enc = torch.Size([1, 4, 100])\n",
            "after decoder = torch.Size([1, 4, 18])\n",
            "after softmax = torch.Size([1, 4, 18])\n",
            "\n",
            "Source: torch.Size([4, 3])\n",
            "after embedding = torch.Size([4, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 3, 100])\n",
            "after positional enc = torch.Size([4, 3, 100])\n",
            "after transformer enc = torch.Size([4, 3, 100])\n",
            "after decoder = torch.Size([4, 3, 18])\n",
            "after softmax = torch.Size([4, 3, 18])\n",
            "\n",
            "Source: torch.Size([1, 3])\n",
            "after embedding = torch.Size([1, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 3, 100])\n",
            "after positional enc = torch.Size([1, 3, 100])\n",
            "after transformer enc = torch.Size([1, 3, 100])\n",
            "after decoder = torch.Size([1, 3, 18])\n",
            "after softmax = torch.Size([1, 3, 18])\n",
            "torch.Size([6, 3])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  2.52 | valid ppl    12.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "Source: torch.Size([4, 4])\n",
            "after embedding = torch.Size([4, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 4, 100])\n",
            "after positional enc = torch.Size([4, 4, 100])\n",
            "after transformer enc = torch.Size([4, 4, 100])\n",
            "after decoder = torch.Size([4, 4, 18])\n",
            "after softmax = torch.Size([4, 4, 18])\n",
            "\n",
            "Source: torch.Size([1, 4])\n",
            "after embedding = torch.Size([1, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 4, 100])\n",
            "after positional enc = torch.Size([1, 4, 100])\n",
            "after transformer enc = torch.Size([1, 4, 100])\n",
            "after decoder = torch.Size([1, 4, 18])\n",
            "after softmax = torch.Size([1, 4, 18])\n",
            "\n",
            "Source: torch.Size([4, 3])\n",
            "after embedding = torch.Size([4, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 3, 100])\n",
            "after positional enc = torch.Size([4, 3, 100])\n",
            "after transformer enc = torch.Size([4, 3, 100])\n",
            "after decoder = torch.Size([4, 3, 18])\n",
            "after softmax = torch.Size([4, 3, 18])\n",
            "\n",
            "Source: torch.Size([1, 3])\n",
            "after embedding = torch.Size([1, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 3, 100])\n",
            "after positional enc = torch.Size([1, 3, 100])\n",
            "after transformer enc = torch.Size([1, 3, 100])\n",
            "after decoder = torch.Size([1, 3, 18])\n",
            "after softmax = torch.Size([1, 3, 18])\n",
            "torch.Size([6, 3])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  2.52 | valid ppl    12.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "Source: torch.Size([4, 4])\n",
            "after embedding = torch.Size([4, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 4, 100])\n",
            "after positional enc = torch.Size([4, 4, 100])\n",
            "after transformer enc = torch.Size([4, 4, 100])\n",
            "after decoder = torch.Size([4, 4, 18])\n",
            "after softmax = torch.Size([4, 4, 18])\n",
            "\n",
            "Source: torch.Size([1, 4])\n",
            "after embedding = torch.Size([1, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 4, 100])\n",
            "after positional enc = torch.Size([1, 4, 100])\n",
            "after transformer enc = torch.Size([1, 4, 100])\n",
            "after decoder = torch.Size([1, 4, 18])\n",
            "after softmax = torch.Size([1, 4, 18])\n",
            "\n",
            "Source: torch.Size([4, 3])\n",
            "after embedding = torch.Size([4, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 3, 100])\n",
            "after positional enc = torch.Size([4, 3, 100])\n",
            "after transformer enc = torch.Size([4, 3, 100])\n",
            "after decoder = torch.Size([4, 3, 18])\n",
            "after softmax = torch.Size([4, 3, 18])\n",
            "\n",
            "Source: torch.Size([1, 3])\n",
            "after embedding = torch.Size([1, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 3, 100])\n",
            "after positional enc = torch.Size([1, 3, 100])\n",
            "after transformer enc = torch.Size([1, 3, 100])\n",
            "after decoder = torch.Size([1, 3, 18])\n",
            "after softmax = torch.Size([1, 3, 18])\n",
            "torch.Size([6, 3])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  2.52 | valid ppl    12.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "Source: torch.Size([4, 4])\n",
            "after embedding = torch.Size([4, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 4, 100])\n",
            "after positional enc = torch.Size([4, 4, 100])\n",
            "after transformer enc = torch.Size([4, 4, 100])\n",
            "after decoder = torch.Size([4, 4, 18])\n",
            "after softmax = torch.Size([4, 4, 18])\n",
            "\n",
            "Source: torch.Size([1, 4])\n",
            "after embedding = torch.Size([1, 4, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 4, 100])\n",
            "after positional enc = torch.Size([1, 4, 100])\n",
            "after transformer enc = torch.Size([1, 4, 100])\n",
            "after decoder = torch.Size([1, 4, 18])\n",
            "after softmax = torch.Size([1, 4, 18])\n",
            "\n",
            "Source: torch.Size([4, 3])\n",
            "after embedding = torch.Size([4, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 3, 100])\n",
            "after positional enc = torch.Size([4, 3, 100])\n",
            "after transformer enc = torch.Size([4, 3, 100])\n",
            "after decoder = torch.Size([4, 3, 18])\n",
            "after softmax = torch.Size([4, 3, 18])\n",
            "\n",
            "Source: torch.Size([1, 3])\n",
            "after embedding = torch.Size([1, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 3, 100])\n",
            "after positional enc = torch.Size([1, 3, 100])\n",
            "after transformer enc = torch.Size([1, 3, 100])\n",
            "after decoder = torch.Size([1, 3, 18])\n",
            "after softmax = torch.Size([1, 3, 18])\n",
            "torch.Size([6, 3])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  2.52 | valid ppl    12.44\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jKXJ5WhNhZM1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "457a3d0f-a589-4756-9b56-27eb0ed1fae3"
      },
      "source": [
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Source: torch.Size([4, 3])\n",
            "after embedding = torch.Size([4, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([4, 3, 100])\n",
            "after positional enc = torch.Size([4, 3, 100])\n",
            "after transformer enc = torch.Size([4, 3, 100])\n",
            "after decoder = torch.Size([4, 3, 18])\n",
            "after softmax = torch.Size([4, 3, 18])\n",
            "\n",
            "Source: torch.Size([1, 3])\n",
            "after embedding = torch.Size([1, 3, 100])\n",
            "Multiplier = 10.0\n",
            "after mult with embedding = torch.Size([1, 3, 100])\n",
            "after positional enc = torch.Size([1, 3, 100])\n",
            "after transformer enc = torch.Size([1, 3, 100])\n",
            "after decoder = torch.Size([1, 3, 18])\n",
            "after softmax = torch.Size([1, 3, 18])\n",
            "torch.Size([6, 3])\n",
            "=========================================================================================\n",
            "| End of training | test loss  2.52 | test ppl    12.44\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyY-Gy-NEZD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}