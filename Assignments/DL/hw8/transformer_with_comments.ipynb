{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3Lzcs3mewEd"
   },
   "source": [
    "\n",
    "# Deep Learning CS6073 Assignment 8\n",
    "\n",
    "    By Akhil Kanna Devarashetti\n",
    "    \n",
    "### Question:\n",
    "\n",
    "    This programming assignment is based on https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "    But we will only run the Transformer.\n",
    "    Download train.txt, valid.txt, and test.txt to ./data/wikitext-2/.\n",
    "    You may need to run python main.py with specification of the selection of Transformer, or python main2.py, \n",
    "    which along with model2.py, is a simplified version only for the Transformer and with a few epochs.\n",
    "    We need data.py to start\n",
    "    and generate.py to show the learning result.\n",
    "    Show that you indeed have spent time in studying and running the programs.\n",
    "    \n",
    "_____\n",
    " \n",
    " To show that I studied the program and transformer model, I'm going to add comments to each part of the program.\n",
    " I divided the complete program into separate parts here so that I can describe what is happening at each step.\n",
    " \n",
    " The whole point of this code is to predict the next word given a sequence.\n",
    " Transformer model can do it quite well without using recurrent architecture.\n",
    " \n",
    " In this notebook, I am executing a model with a very small dataset to check how things work.\n",
    " But, the execution of the model with the complete dataset is present in the *other notebook* titled `full_execution.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q19y8dI2aY-J",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from io import open\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuwsKeh4e_Wv",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "bptt = 4 # Reduced from 20 to check how it works.\n",
    "loginterval = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset into `data/wikitext-2` using bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "hJ0XwFDDffot",
    "outputId": "592f3098-dca7-43ad-c92f-4e761b9fd329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1227k  100 1227k    0     0  4445k      0 --:--:-- --:--:-- --:--:-- 4445k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10.2M  100 10.2M    0     0  27.2M      0 --:--:-- --:--:-- --:--:-- 27.2M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1095k  100 1095k    0     0  4416k      0 --:--:-- --:--:-- --:--:-- 4416k\n",
      "test.txt  train.txt  valid.txt\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/wikitext-2\n",
    "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt > data/wikitext-2/test.txt\n",
    "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt > data/wikitext-2/train.txt\n",
    "!curl https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt > data/wikitext-2/valid.txt\n",
    "!ls data/wikitext-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIrSQgI0H109"
   },
   "source": [
    "## Lets take a small dataset to analyse what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "uualMPRYAy5U",
    "outputId": "7d204566-50da-4dbc-fb3c-1eae8fc729d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "└── wikitext-2\n",
      "    ├── test.txt\n",
      "    ├── train.txt\n",
      "    └── valid.txt\n",
      "\n",
      "1 directory, 3 files\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/sample/\n",
    "!echo -e 'a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox' > data/wikitext-2/test.txt\n",
    "!echo -e \"the quick brown fox moves faster than the lazy dog\\nthis is because the quick brown fox is smarter than the lazy dog\" > data/wikitext-2/train.txt\n",
    "!echo -e \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox\" > data/wikitext-2/valid.txt\n",
    "# !apt-get install tree  # If tree doesn't work\n",
    "!tree data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iqQq0ZKhYLkn",
    "outputId": "6b04fb80-c781-40eb-c0e3-8b78bdc809fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox moves faster than the lazy dog\n",
      "this is because the quick brown fox is smarter than the lazy dog\n"
     ]
    }
   ],
   "source": [
    "!cat data/sample/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kV7nXFvNIaJJ"
   },
   "source": [
    "## The dictionary class saves the mapping between the words and their indices.\n",
    "## Every word is assigned an integer based on the order of its occurance.\n",
    "\n",
    "    Example input: \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox <eos>\"\n",
    "\n",
    "    Output: \n",
    "    {'a': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'the': 6, 'lazy': 7, 'dog': 8, 'is': 9, 'not': 10, '<eos>': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnN3pZAWIZKr",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KFigxgkpRB-1",
    "outputId": "5fd054ea-e8fa-4663-eb59-1485179257ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'jumps': 4, 'over': 5, 'the': 6, 'lazy': 7, 'dog': 8, 'is': 9, 'not': 10, '<eos>': 11}\n"
     ]
    }
   ],
   "source": [
    "sample_input = \"a quick brown fox jumps over the lazy dog\\na lazy dog is not a brown fox <eos>\"\n",
    "\n",
    "sample_dict = Dictionary()\n",
    "\n",
    "for word in sample_input.split():\n",
    "    sample_dict.add_word(word)\n",
    "\n",
    "print(sample_dict.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-i09vm9cSVu9"
   },
   "source": [
    "### Corpus class will create the dictionary for all three datasets.\n",
    "\n",
    "    For example:\n",
    "    test data = \"a quick brown fox jumps over the lazy dog a lazy dog is not a brown fox\"\n",
    "    corpus.test = tensor([14,  1,  2,  3, 15, 16,  0,  7, 17,  7, 12,  9, 18, 14,  2,  3, 13])\n",
    "\n",
    "Note that 'a' is repeated twice and we see it's id 14 repeated twice too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ksgpgsifz1j",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqJUr8gig0Dc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 4 # 20\n",
    "eval_batch_size = 3 # 10\n",
    "\n",
    "#corpus = Corpus('./data/wikitext-2')\n",
    "corpus = Corpus('./data/sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "kEHBU_Q9XLxb",
    "outputId": "f1a3af20-887e-40b9-accd-4d3168a14f17",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: {'the': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'moves': 4, 'faster': 5, 'than': 6, 'lazy': 7, 'dog': 8, '<eos>': 9, 'this': 10, 'is': 11, 'because': 12, 'smarter': 13, 'a': 14, 'jumps': 15, 'over': 16, 'not': 17}\n",
      "\n",
      "Test data:\n",
      "\n",
      "corpus.test: tensor([14,  1,  2,  3, 15, 16,  0,  7,  8,  9, 14,  7,  8, 11, 17, 14,  2,  3,\n",
      "         9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "print(f'Dictionary: {corpus.dictionary.word2idx}')\n",
    "print('\\nTest data:')\n",
    "!cat data/wikitext-2/test.txt\n",
    "print(f'\\ncorpus.test: {corpus.test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbOPYIxuQnVF"
   },
   "source": [
    "### As given in the documentation, the batchify function will take the complete dataset as input string and converts it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFt9nrGgQYp9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "HbnyLS9tVGHi",
    "outputId": "7cd4dace-c90d-417b-e311-c42624fa1586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14,  1,  2,  3, 15, 16,  0,  7,  8,  9, 14,  7,  8, 11, 17, 14,  2,  3,\n",
      "         9])\n",
      "tensor([[14,  0,  8],\n",
      "        [ 1,  7, 11],\n",
      "        [ 2,  8, 17],\n",
      "        [ 3,  9, 14],\n",
      "        [15, 14,  2],\n",
      "        [16,  7,  3]])\n"
     ]
    }
   ],
   "source": [
    "print(corpus.test)\n",
    "sample_batches = batchify(corpus.test, 3)\n",
    "print(sample_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_0DTcZUvm1i"
   },
   "source": [
    "### get_batch will actually create a batch of sequence size bptt from index i. Source is the output from batchify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4A68NRSQfIP",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    target = source[i + 1:i + 1 + seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "B3ZwPYXSa4KI",
    "outputId": "86b4c942-1ea3-42d9-dd8c-e493d7eacdb9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[14,  0,  8],\n",
      "        [ 1,  7, 11],\n",
      "        [ 2,  8, 17],\n",
      "        [ 3,  9, 14]]), tensor([ 1,  7, 11,  2,  8, 17,  3,  9, 14, 15, 14,  2]))\n",
      "(tensor([[ 1,  7, 11],\n",
      "        [ 2,  8, 17],\n",
      "        [ 3,  9, 14],\n",
      "        [15, 14,  2]]), tensor([ 2,  8, 17,  3,  9, 14, 15, 14,  2, 16,  7,  3]))\n"
     ]
    }
   ],
   "source": [
    "print(get_batch(sample_batches, 0))\n",
    "print(get_batch(sample_batches, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktYWnHTBQVjU",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gg0WElg70T",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word_idx: \n",
      "{'the': 0, 'quick': 1, 'brown': 2, 'fox': 3, 'moves': 4, 'faster': 5, 'than': 6, 'lazy': 7, 'dog': 8, '<eos>': 9, 'this': 10, 'is': 11, 'because': 12, 'smarter': 13, 'a': 14, 'jumps': 15, 'over': 16, 'not': 17}\n",
      "\n",
      "Train data: torch.Size([6, 4])\n",
      "tensor([[ 0,  6, 11, 11],\n",
      "        [ 1,  0, 12, 13],\n",
      "        [ 2,  7,  0,  6],\n",
      "        [ 3,  8,  1,  0],\n",
      "        [ 4,  9,  2,  7],\n",
      "        [ 5, 10,  3,  8]])\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nword_idx: \\n{corpus.dictionary.word2idx}')\n",
    "print(f'\\nTrain data: {train_data.shape}\\n{train_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below is the implementation of Positional Encoder in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The cell below is the implementation of Transformer network. It consists of the following flow for a *SINGLE* batch:\n",
    "\n",
    "## 1. Input:\n",
    "Each sample in a batch is an array of size S where each element is an integer which is an ID of a word in the vocabulary.\n",
    "\n",
    "Input shape: (S)  (S = sequence length = bptt = 20)\n",
    "\n",
    "Example: [1, 45, 3, ...]\n",
    "\n",
    "## 2. Embedding:\n",
    "The embedding layer generates a real number array of size E for each integer (word).\n",
    "\n",
    "Output shape: (S, E)   (E = Embedding size) (E = 100)\n",
    "\n",
    "Example input: [(1.2, 0.3, 4.5), (x, x, x), ... (x, x, x)] For E = 3\n",
    "\n",
    "## 3. Positional Encoding:\n",
    "Positional encoding will encode the relative positions of the words in the sequence.\n",
    "\n",
    "Output shape: (S, E)\n",
    "\n",
    "Example input: [(1.2, 0.3, 4.5), (x, x, x), ... (x, x, x)] For E = 3\n",
    "\n",
    "## 4. Transformer Encoding:\n",
    "This module does the rest of the transformer professes with keys, values etc internally.\n",
    "\n",
    "Output shape: (S, E)\n",
    "\n",
    "Example input: [(1.2, 0.3, 4.5), (x, x, x), ... (x, x, x)] For E = 3\n",
    "\n",
    "## 5. Decoder:\n",
    "This is a fully-connected layer that outputs a vector size of vocabulary V for each embedding.\n",
    "\n",
    "Output shape: (S, V)  (V = Vocabulary size)\n",
    "\n",
    "Example input: [(1.2, 0.3), (x, x), ... (x, x)] For V = 2\n",
    "\n",
    "## 6. Softmax:\n",
    "This module converts each of the V size arrays to probability distributions. The word with the highest probability will be the next word.\n",
    "\n",
    "Output shape: (S, V)  \n",
    "\n",
    "Example input: [(0.7, 0.3), (x, x), ... (x, x)] For V = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        print(f'\\nSource: {src.shape}')\n",
    "        src = self.encoder(src)\n",
    "        print(f'after embedding = {src.shape}')\n",
    "        src = src * math.sqrt(self.ninp)\n",
    "        print(f'Multiplier = {math.sqrt(self.ninp)}')\n",
    "        print(f'after mult with embedding = {src.shape}')\n",
    "        src = self.pos_encoder(src)\n",
    "        print(f'after positional enc = {src.shape}')\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        print(f'after transformer enc = {output.shape}')\n",
    "        output = self.decoder(output)\n",
    "        print(f'after decoder = {output.shape}')\n",
    "        softmax_output = F.log_softmax(output, dim=-1)\n",
    "        print(f'after softmax = {softmax_output.shape}')\n",
    "        return softmax_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the values for the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhhalf99hLr5",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "emsize = 100  # E = Embedding size\n",
    "nhead = 2     # Number of heads in the transformer\n",
    "nhid = 64     # Hidden units size in the transformer\n",
    "nlayers = 2\n",
    "dropout = 0.2\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2GxEh8ZhU-x",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % loginterval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / loginterval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPI3C9qdz-_1",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    print(data_source.shape)\n",
    "    return total_loss / (len(data_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "Ma2Je00AhW_z",
    "outputId": "5e368534-9f29-430b-963b-dddcc448c83b",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source: torch.Size([4, 4])\n",
      "after embedding = torch.Size([4, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 4, 100])\n",
      "after positional enc = torch.Size([4, 4, 100])\n",
      "after transformer enc = torch.Size([4, 4, 100])\n",
      "after decoder = torch.Size([4, 4, 18])\n",
      "after softmax = torch.Size([4, 4, 18])\n",
      "\n",
      "Source: torch.Size([1, 4])\n",
      "after embedding = torch.Size([1, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 4, 100])\n",
      "after positional enc = torch.Size([1, 4, 100])\n",
      "after transformer enc = torch.Size([1, 4, 100])\n",
      "after decoder = torch.Size([1, 4, 18])\n",
      "after softmax = torch.Size([1, 4, 18])\n",
      "\n",
      "Source: torch.Size([4, 3])\n",
      "after embedding = torch.Size([4, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 3, 100])\n",
      "after positional enc = torch.Size([4, 3, 100])\n",
      "after transformer enc = torch.Size([4, 3, 100])\n",
      "after decoder = torch.Size([4, 3, 18])\n",
      "after softmax = torch.Size([4, 3, 18])\n",
      "\n",
      "Source: torch.Size([1, 3])\n",
      "after embedding = torch.Size([1, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 3, 100])\n",
      "after positional enc = torch.Size([1, 3, 100])\n",
      "after transformer enc = torch.Size([1, 3, 100])\n",
      "after decoder = torch.Size([1, 3, 18])\n",
      "after softmax = torch.Size([1, 3, 18])\n",
      "torch.Size([6, 3])\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  2.41 | valid ppl    11.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Source: torch.Size([4, 4])\n",
      "after embedding = torch.Size([4, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 4, 100])\n",
      "after positional enc = torch.Size([4, 4, 100])\n",
      "after transformer enc = torch.Size([4, 4, 100])\n",
      "after decoder = torch.Size([4, 4, 18])\n",
      "after softmax = torch.Size([4, 4, 18])\n",
      "\n",
      "Source: torch.Size([1, 4])\n",
      "after embedding = torch.Size([1, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 4, 100])\n",
      "after positional enc = torch.Size([1, 4, 100])\n",
      "after transformer enc = torch.Size([1, 4, 100])\n",
      "after decoder = torch.Size([1, 4, 18])\n",
      "after softmax = torch.Size([1, 4, 18])\n",
      "\n",
      "Source: torch.Size([4, 3])\n",
      "after embedding = torch.Size([4, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 3, 100])\n",
      "after positional enc = torch.Size([4, 3, 100])\n",
      "after transformer enc = torch.Size([4, 3, 100])\n",
      "after decoder = torch.Size([4, 3, 18])\n",
      "after softmax = torch.Size([4, 3, 18])\n",
      "\n",
      "Source: torch.Size([1, 3])\n",
      "after embedding = torch.Size([1, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 3, 100])\n",
      "after positional enc = torch.Size([1, 3, 100])\n",
      "after transformer enc = torch.Size([1, 3, 100])\n",
      "after decoder = torch.Size([1, 3, 18])\n",
      "after softmax = torch.Size([1, 3, 18])\n",
      "torch.Size([6, 3])\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  2.41 | valid ppl    11.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Source: torch.Size([4, 4])\n",
      "after embedding = torch.Size([4, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 4, 100])\n",
      "after positional enc = torch.Size([4, 4, 100])\n",
      "after transformer enc = torch.Size([4, 4, 100])\n",
      "after decoder = torch.Size([4, 4, 18])\n",
      "after softmax = torch.Size([4, 4, 18])\n",
      "\n",
      "Source: torch.Size([1, 4])\n",
      "after embedding = torch.Size([1, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 4, 100])\n",
      "after positional enc = torch.Size([1, 4, 100])\n",
      "after transformer enc = torch.Size([1, 4, 100])\n",
      "after decoder = torch.Size([1, 4, 18])\n",
      "after softmax = torch.Size([1, 4, 18])\n",
      "\n",
      "Source: torch.Size([4, 3])\n",
      "after embedding = torch.Size([4, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 3, 100])\n",
      "after positional enc = torch.Size([4, 3, 100])\n",
      "after transformer enc = torch.Size([4, 3, 100])\n",
      "after decoder = torch.Size([4, 3, 18])\n",
      "after softmax = torch.Size([4, 3, 18])\n",
      "\n",
      "Source: torch.Size([1, 3])\n",
      "after embedding = torch.Size([1, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 3, 100])\n",
      "after positional enc = torch.Size([1, 3, 100])\n",
      "after transformer enc = torch.Size([1, 3, 100])\n",
      "after decoder = torch.Size([1, 3, 18])\n",
      "after softmax = torch.Size([1, 3, 18])\n",
      "torch.Size([6, 3])\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  2.41 | valid ppl    11.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Source: torch.Size([4, 4])\n",
      "after embedding = torch.Size([4, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 4, 100])\n",
      "after positional enc = torch.Size([4, 4, 100])\n",
      "after transformer enc = torch.Size([4, 4, 100])\n",
      "after decoder = torch.Size([4, 4, 18])\n",
      "after softmax = torch.Size([4, 4, 18])\n",
      "\n",
      "Source: torch.Size([1, 4])\n",
      "after embedding = torch.Size([1, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 4, 100])\n",
      "after positional enc = torch.Size([1, 4, 100])\n",
      "after transformer enc = torch.Size([1, 4, 100])\n",
      "after decoder = torch.Size([1, 4, 18])\n",
      "after softmax = torch.Size([1, 4, 18])\n",
      "\n",
      "Source: torch.Size([4, 3])\n",
      "after embedding = torch.Size([4, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 3, 100])\n",
      "after positional enc = torch.Size([4, 3, 100])\n",
      "after transformer enc = torch.Size([4, 3, 100])\n",
      "after decoder = torch.Size([4, 3, 18])\n",
      "after softmax = torch.Size([4, 3, 18])\n",
      "\n",
      "Source: torch.Size([1, 3])\n",
      "after embedding = torch.Size([1, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 3, 100])\n",
      "after positional enc = torch.Size([1, 3, 100])\n",
      "after transformer enc = torch.Size([1, 3, 100])\n",
      "after decoder = torch.Size([1, 3, 18])\n",
      "after softmax = torch.Size([1, 3, 18])\n",
      "torch.Size([6, 3])\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  2.41 | valid ppl    11.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Source: torch.Size([4, 4])\n",
      "after embedding = torch.Size([4, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 4, 100])\n",
      "after positional enc = torch.Size([4, 4, 100])\n",
      "after transformer enc = torch.Size([4, 4, 100])\n",
      "after decoder = torch.Size([4, 4, 18])\n",
      "after softmax = torch.Size([4, 4, 18])\n",
      "\n",
      "Source: torch.Size([1, 4])\n",
      "after embedding = torch.Size([1, 4, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 4, 100])\n",
      "after positional enc = torch.Size([1, 4, 100])\n",
      "after transformer enc = torch.Size([1, 4, 100])\n",
      "after decoder = torch.Size([1, 4, 18])\n",
      "after softmax = torch.Size([1, 4, 18])\n",
      "\n",
      "Source: torch.Size([4, 3])\n",
      "after embedding = torch.Size([4, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 3, 100])\n",
      "after positional enc = torch.Size([4, 3, 100])\n",
      "after transformer enc = torch.Size([4, 3, 100])\n",
      "after decoder = torch.Size([4, 3, 18])\n",
      "after softmax = torch.Size([4, 3, 18])\n",
      "\n",
      "Source: torch.Size([1, 3])\n",
      "after embedding = torch.Size([1, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 3, 100])\n",
      "after positional enc = torch.Size([1, 3, 100])\n",
      "after transformer enc = torch.Size([1, 3, 100])\n",
      "after decoder = torch.Size([1, 3, 18])\n",
      "after softmax = torch.Size([1, 3, 18])\n",
      "torch.Size([6, 3])\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  2.41 | valid ppl    11.13\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "lr = 20\n",
    "best_val_loss = None\n",
    "epochs = 5\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open('model.pt', 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKXJ5WhNhZM1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source: torch.Size([4, 3])\n",
      "after embedding = torch.Size([4, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([4, 3, 100])\n",
      "after positional enc = torch.Size([4, 3, 100])\n",
      "after transformer enc = torch.Size([4, 3, 100])\n",
      "after decoder = torch.Size([4, 3, 18])\n",
      "after softmax = torch.Size([4, 3, 18])\n",
      "\n",
      "Source: torch.Size([1, 3])\n",
      "after embedding = torch.Size([1, 3, 100])\n",
      "Multiplier = 10.0\n",
      "after mult with embedding = torch.Size([1, 3, 100])\n",
      "after positional enc = torch.Size([1, 3, 100])\n",
      "after transformer enc = torch.Size([1, 3, 100])\n",
      "after decoder = torch.Size([1, 3, 18])\n",
      "after softmax = torch.Size([1, 3, 18])\n",
      "torch.Size([6, 3])\n",
      "=========================================================================================\n",
      "| End of training | test loss  2.41 | test ppl    11.13\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "AkkiTest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
