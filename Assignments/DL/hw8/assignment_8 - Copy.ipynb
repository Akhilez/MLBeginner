{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning CS6073 Assignment 8\n",
    "\n",
    "    By Akhil Kanna Devarashetti\n",
    "    \n",
    "Question:\n",
    "\n",
    "    This programming assignment is based on https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "    But we will only run the Transformer.\n",
    "    Download train.txt, valid.txt, and test.txt to ./data/wikitext-2/.\n",
    "    You may need to run python main.py with specification of the selection of Transformer, or python main2.py, \n",
    "    which along with model2.py, is a simplified version only for the Transformer and with a few epochs.\n",
    "    We need data.py to start\n",
    "    and generate.py to show the learning result.\n",
    "    Show that you indeed have spent time in studying and running the programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import data\n",
    "import model2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "bptt = 20\n",
    "loginterval = 200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    target = source[i + 1:i + 1 + seq_len].view(-1)\n",
    "    return data, target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "\n",
    "corpus = data.Corpus('./data/wikitext-2')\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "emsize = 100\n",
    "nhead = 2\n",
    "nhid = 64\n",
    "nlayers = 2\n",
    "dropout = 0.2\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = model2.TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "criterion = nn.NLLLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % loginterval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / loginterval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "| epoch   1 |   200/ 5221 batches | lr 20.00 | loss 10.67 | ppl 42922.85\n",
      "| epoch   1 |   400/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40346.76\n",
      "| epoch   1 |   600/ 5221 batches | lr 20.00 | loss 10.62 | ppl 40875.39\n",
      "| epoch   1 |   800/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40327.92\n",
      "| epoch   1 |  1000/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40474.58\n",
      "| epoch   1 |  1200/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40287.29\n",
      "| epoch   1 |  1400/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40642.39\n",
      "| epoch   1 |  1600/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40576.35\n",
      "| epoch   1 |  1800/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40575.26\n",
      "| epoch   1 |  2000/ 5221 batches | lr 20.00 | loss 10.62 | ppl 40744.91\n",
      "| epoch   1 |  2200/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40584.58\n",
      "| epoch   1 |  2400/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40543.02\n",
      "| epoch   1 |  2600/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40302.77\n",
      "| epoch   1 |  2800/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40408.35\n",
      "| epoch   1 |  3000/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40585.51\n",
      "| epoch   1 |  3200/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40603.39\n",
      "| epoch   1 |  3400/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40560.45\n",
      "| epoch   1 |  3600/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40619.43\n",
      "| epoch   1 |  3800/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40490.72\n",
      "| epoch   1 |  4000/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40532.40\n",
      "| epoch   1 |  4200/ 5221 batches | lr 20.00 | loss 10.62 | ppl 40793.26\n",
      "| epoch   1 |  4400/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40250.43\n",
      "| epoch   1 |  4600/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40405.99\n",
      "| epoch   1 |  4800/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40449.98\n",
      "| epoch   1 |  5000/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40556.75\n",
      "| epoch   1 |  5200/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40637.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 10.61 | valid ppl 40363.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 5221 batches | lr 20.00 | loss 10.67 | ppl 42965.82\n",
      "| epoch   2 |   400/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40413.02\n",
      "| epoch   2 |   600/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40736.00\n",
      "| epoch   2 |   800/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40324.17\n",
      "| epoch   2 |  1000/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40450.66\n",
      "| epoch   2 |  1200/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40295.87\n",
      "| epoch   2 |  1400/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40691.09\n",
      "| epoch   2 |  1600/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40577.35\n",
      "| epoch   2 |  1800/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40554.44\n",
      "| epoch   2 |  2000/ 5221 batches | lr 20.00 | loss 10.62 | ppl 40823.60\n",
      "| epoch   2 |  2200/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40595.80\n",
      "| epoch   2 |  2400/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40522.25\n",
      "| epoch   2 |  2600/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40153.34\n",
      "| epoch   2 |  2800/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40276.12\n",
      "| epoch   2 |  3000/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40595.89\n",
      "| epoch   2 |  3200/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40526.21\n",
      "| epoch   2 |  3400/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40621.25\n",
      "| epoch   2 |  3600/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40617.53\n",
      "| epoch   2 |  3800/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40466.91\n",
      "| epoch   2 |  4000/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40537.61\n",
      "| epoch   2 |  4200/ 5221 batches | lr 20.00 | loss 10.62 | ppl 40793.77\n",
      "| epoch   2 |  4400/ 5221 batches | lr 20.00 | loss 10.60 | ppl 40229.33\n",
      "| epoch   2 |  4600/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40375.20\n",
      "| epoch   2 |  4800/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40492.42\n",
      "| epoch   2 |  5000/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40513.91\n",
      "| epoch   2 |  5200/ 5221 batches | lr 20.00 | loss 10.61 | ppl 40585.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 10.61 | valid ppl 40363.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 5221 batches | lr 5.00 | loss 10.67 | ppl 42916.03\n",
      "| epoch   3 |   400/ 5221 batches | lr 5.00 | loss 10.60 | ppl 40318.31\n",
      "| epoch   3 |   600/ 5221 batches | lr 5.00 | loss 10.62 | ppl 40744.04\n",
      "| epoch   3 |   800/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40381.06\n",
      "| epoch   3 |  1000/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40414.97\n",
      "| epoch   3 |  1200/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40380.05\n",
      "| epoch   3 |  1400/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40688.10\n",
      "| epoch   3 |  1600/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40466.61\n",
      "| epoch   3 |  1800/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40562.44\n",
      "| epoch   3 |  2000/ 5221 batches | lr 5.00 | loss 10.62 | ppl 40763.46\n",
      "| epoch   3 |  2200/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40570.15\n",
      "| epoch   3 |  2400/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40540.23\n",
      "| epoch   3 |  2600/ 5221 batches | lr 5.00 | loss 10.60 | ppl 40283.36\n",
      "| epoch   3 |  2800/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40390.45\n",
      "| epoch   3 |  3000/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40623.54\n",
      "| epoch   3 |  3200/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40606.21\n",
      "| epoch   3 |  3400/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40589.60\n",
      "| epoch   3 |  3600/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40637.51\n",
      "| epoch   3 |  3800/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40479.49\n",
      "| epoch   3 |  4000/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40481.29\n",
      "| epoch   3 |  4200/ 5221 batches | lr 5.00 | loss 10.62 | ppl 40751.29\n",
      "| epoch   3 |  4400/ 5221 batches | lr 5.00 | loss 10.60 | ppl 40247.89\n",
      "| epoch   3 |  4600/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40337.72\n",
      "| epoch   3 |  4800/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40399.05\n",
      "| epoch   3 |  5000/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40614.99\n",
      "| epoch   3 |  5200/ 5221 batches | lr 5.00 | loss 10.61 | ppl 40637.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 10.61 | valid ppl 40363.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 5221 batches | lr 1.25 | loss 10.67 | ppl 42895.72\n",
      "| epoch   4 |   400/ 5221 batches | lr 1.25 | loss 10.60 | ppl 40333.24\n",
      "| epoch   4 |   600/ 5221 batches | lr 1.25 | loss 10.62 | ppl 40759.70\n",
      "| epoch   4 |   800/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40386.07\n",
      "| epoch   4 |  1000/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40482.58\n",
      "| epoch   4 |  1200/ 5221 batches | lr 1.25 | loss 10.60 | ppl 40305.17\n",
      "| epoch   4 |  1400/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40694.78\n",
      "| epoch   4 |  1600/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40578.67\n",
      "| epoch   4 |  1800/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40591.16\n",
      "| epoch   4 |  2000/ 5221 batches | lr 1.25 | loss 10.62 | ppl 40840.38\n",
      "| epoch   4 |  2200/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40574.54\n",
      "| epoch   4 |  2400/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40637.47\n",
      "| epoch   4 |  2600/ 5221 batches | lr 1.25 | loss 10.60 | ppl 40268.38\n",
      "| epoch   4 |  2800/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40437.32\n",
      "| epoch   4 |  3000/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40601.07\n",
      "| epoch   4 |  3200/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40603.11\n",
      "| epoch   4 |  3400/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40558.42\n",
      "| epoch   4 |  3600/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40625.50\n",
      "| epoch   4 |  3800/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40411.08\n",
      "| epoch   4 |  4000/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40533.83\n",
      "| epoch   4 |  4200/ 5221 batches | lr 1.25 | loss 10.62 | ppl 40815.58\n",
      "| epoch   4 |  4400/ 5221 batches | lr 1.25 | loss 10.60 | ppl 40269.84\n",
      "| epoch   4 |  4600/ 5221 batches | lr 1.25 | loss 10.60 | ppl 40326.86\n",
      "| epoch   4 |  4800/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40499.29\n",
      "| epoch   4 |  5000/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40546.85\n",
      "| epoch   4 |  5200/ 5221 batches | lr 1.25 | loss 10.61 | ppl 40630.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 10.61 | valid ppl 40363.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 5221 batches | lr 0.31 | loss 10.67 | ppl 42863.17\n",
      "| epoch   5 |   400/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40409.22\n",
      "| epoch   5 |   600/ 5221 batches | lr 0.31 | loss 10.62 | ppl 40858.97\n",
      "| epoch   5 |   800/ 5221 batches | lr 0.31 | loss 10.60 | ppl 40310.29\n",
      "| epoch   5 |  1000/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40443.48\n",
      "| epoch   5 |  1200/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40369.25\n",
      "| epoch   5 |  1400/ 5221 batches | lr 0.31 | loss 10.62 | ppl 40782.03\n",
      "| epoch   5 |  1600/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40663.68\n",
      "| epoch   5 |  1800/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40612.11\n",
      "| epoch   5 |  2000/ 5221 batches | lr 0.31 | loss 10.62 | ppl 40795.36\n",
      "| epoch   5 |  2200/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40563.89\n",
      "| epoch   5 |  2400/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40511.07\n",
      "| epoch   5 |  2600/ 5221 batches | lr 0.31 | loss 10.60 | ppl 40258.90\n",
      "| epoch   5 |  2800/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40455.73\n",
      "| epoch   5 |  3000/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40582.21\n",
      "| epoch   5 |  3200/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40589.46\n",
      "| epoch   5 |  3400/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40627.81\n",
      "| epoch   5 |  3600/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40623.11\n",
      "| epoch   5 |  3800/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40483.78\n",
      "| epoch   5 |  4000/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40548.94\n",
      "| epoch   5 |  4200/ 5221 batches | lr 0.31 | loss 10.62 | ppl 40800.22\n",
      "| epoch   5 |  4400/ 5221 batches | lr 0.31 | loss 10.60 | ppl 40230.70\n",
      "| epoch   5 |  4600/ 5221 batches | lr 0.31 | loss 10.60 | ppl 40315.11\n",
      "| epoch   5 |  4800/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40371.04\n",
      "| epoch   5 |  5000/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40455.42\n",
      "| epoch   5 |  5200/ 5221 batches | lr 0.31 | loss 10.61 | ppl 40684.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 10.61 | valid ppl 40363.75\n",
      "-----------------------------------------------------------------------------------------\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "# Loop over epochs.\n",
    "lr = 20\n",
    "best_val_loss = None\n",
    "epochs = 5\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open('model.pt', 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}