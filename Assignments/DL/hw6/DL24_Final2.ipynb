{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment 6 of Deep Learning CS6073\n",
    "    By Akhil Kanna Devarashetti\n",
    "\n",
    "#### Question:\n",
    "\n",
    "    Write a Pytorch version of the Word2vec/skip-gram displayed in Chapter 14 of d2l.  \n",
    "    In particular, make DL24.py error free \n",
    "    Implement get_similar_tokens as an application of the word embedding model \n",
    "    (14.4.3 of d2l and also the last slide in lecture)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# DL24.py CS5173/6073 2020 cheng\n",
    "# making centers, contexts, and negatives for PennTreebank data\n",
    "# building vocabulary, performing subsampling and negative sampling\n",
    "# Skip-gram word embedding as a translation from MXNet to Pytorch of d2l chapter 14\n",
    "# Usage: python DL24.py\n",
    "\n",
    "import zipfile\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile('data/ptb.zip', 'r')\n",
    "raw_text = f.read('ptb/ptb.train.txt').decode(\"utf-8\")\n",
    "sentences = [line.split() for line in raw_text.split('\\n')]\n",
    "tokens = [tk for line in sentences for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "uniq_tokens = [token for token, freq in list(counter.items()) if counter[token] >= 10]\n",
    "idx_to_token, token_to_idx = [], dict()\n",
    "for token in uniq_tokens:\n",
    "    idx_to_token.append(token)\n",
    "    token_to_idx[token] = len(idx_to_token) - 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Replacing the tokens that are < 10 in frequency with token_to_idx[0] token.\n",
    "s = [[idx_to_token[token_to_idx.get(tk, 0)] for tk in line] for line in sentences]\n",
    "tokens = [tk for line in s for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "num_tokens = sum(counter.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Sample the tokens which are rare.\n",
    "order_of_magnitude = round(math.log10(num_tokens))\n",
    "inverse_frequency = num_tokens / (10 ** order_of_magnitude)\n",
    "\n",
    "subsampled = [[tk for tk in line if random.uniform(0, 1) < math.sqrt(inverse_frequency / counter[tk] * num_tokens)] for line in s]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "corpus = [[token_to_idx.get(tk) for tk in line] for line in subsampled]\n",
    "\n",
    "# corpus ~= [[1, 3, 4, 5], [43, 21, 44, 45]] <- indices of words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokens = [tk for line in corpus for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "sampling_weights = [counter[i]**0.75 for i in range(len(counter))]\n",
    "population = list(range(len(sampling_weights)))\n",
    "candidates = random.choices(population, sampling_weights, k=(10 ** order_of_magnitude))\n",
    "# candidates = sampled tokens which occur rarely."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The error was in this loop!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "window_range = 2\n",
    "max_window_size = 5\n",
    "K = 5\n",
    "j = 0\n",
    "data = []\n",
    "maxLen = 0\n",
    "for line in corpus:\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    for i in range(len(line)):  # i is the center word\n",
    "        # Find a list of context \n",
    "        context = []\n",
    "        for w in range(-window_range, window_range + 1):\n",
    "            context_word_pos = i + w\n",
    "            if context_word_pos < 0 or context_word_pos >= len(line) or i == context_word_pos or line[i] == line[context_word_pos]:\n",
    "                continue\n",
    "            context_word_idx = line[context_word_pos]\n",
    "            context.append(context_word_idx)\n",
    "            \n",
    "        if len(context) == 0:\n",
    "            continue\n",
    "\n",
    "        neg = []\n",
    "        while len(neg) < len(context) * K:\n",
    "            ne = candidates[j]\n",
    "            j += 1\n",
    "            if j >= (10 ** order_of_magnitude):\n",
    "                j = 0\n",
    "            if ne not in context:\n",
    "                neg.append(ne)\n",
    "       \n",
    "        data.append([line[i], context, neg])\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "centers, contexts_negatives, labels = [], [], []\n",
    "for center, context, negative in data:\n",
    "    cur_len = len(context) + len(negative)\n",
    "    centers += [center]\n",
    "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class PTBdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(PTBdataset).__init__()\n",
    "        self.centers = np.array(centers).reshape(-1, 1)\n",
    "        self.contexts_negatives = np.array(contexts_negatives)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centers[idx], self.contexts_negatives[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "pdata = PTBdataset()\n",
    "data_iter = torch.utils.data.DataLoader(pdata, batch_size=512, shuffle=True)\n",
    "\n",
    "vocab_size = len(idx_to_token)\n",
    "embed_size = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embed_size),\n",
    "    nn.Embedding(vocab_size, embed_size))\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), 0.01)\n",
    "m = nn.Sigmoid()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 49 13.814745903015137\n",
      "0 99 13.916586875915527\n",
      "0 149 14.155638694763184\n",
      "0 199 13.475334167480469\n",
      "0 249 13.154608726501465\n",
      "0 299 12.832062721252441\n",
      "0 349 13.730950355529785\n",
      "0 399 12.777384757995605\n",
      "0 449 12.270665168762207\n",
      "0 499 12.674140930175781\n",
      "0 549 12.84369945526123\n",
      "0 599 12.818958282470703\n",
      "0 649 12.392891883850098\n",
      "0 699 11.700922966003418\n",
      "0 749 11.615740776062012\n",
      "0 799 12.238015174865723\n",
      "0 849 13.104876518249512\n",
      "0 899 12.056645393371582\n",
      "0 949 10.534405708312988\n",
      "0 999 11.256355285644531\n",
      "0 1049 10.76865291595459\n",
      "0 1099 11.018086433410645\n",
      "0 1149 11.31623363494873\n",
      "0 1199 10.913350105285645\n",
      "0 1249 10.382098197937012\n",
      "0 1299 11.408995628356934\n",
      "0 1349 10.547050476074219\n",
      "0 1399 10.9283447265625\n",
      "0 1449 11.507621765136719\n",
      "0 1499 11.409274101257324\n",
      "0 1549 10.506775856018066\n",
      "0 1599 10.820523262023926\n",
      "0 1649 10.904644966125488\n",
      "0 1699 10.249361991882324\n",
      "\n",
      "Epoch: 1\n",
      "1 49 10.368977546691895\n",
      "1 99 10.40188980102539\n",
      "1 149 10.878504753112793\n",
      "1 199 10.106024742126465\n",
      "1 249 10.026631355285645\n",
      "1 299 10.062199592590332\n",
      "1 349 10.309391021728516\n",
      "1 399 10.894100189208984\n",
      "1 449 9.88329029083252\n",
      "1 499 10.053749084472656\n",
      "1 549 10.05691909790039\n",
      "1 599 10.49592399597168\n",
      "1 649 10.245162963867188\n",
      "1 699 10.106616020202637\n",
      "1 749 10.78017807006836\n",
      "1 799 10.963760375976562\n",
      "1 849 10.387675285339355\n",
      "1 899 10.009305953979492\n",
      "1 949 10.05182933807373\n",
      "1 999 10.219862937927246\n",
      "1 1049 9.926407814025879\n",
      "1 1099 10.0862398147583\n",
      "1 1149 9.772110939025879\n",
      "1 1199 9.804280281066895\n",
      "1 1249 9.63952350616455\n",
      "1 1299 9.678959846496582\n",
      "1 1349 9.494791984558105\n",
      "1 1399 9.360676765441895\n",
      "1 1449 9.847821235656738\n",
      "1 1499 9.81990909576416\n",
      "1 1549 9.972867965698242\n",
      "1 1599 9.215643882751465\n",
      "1 1649 10.088290214538574\n",
      "1 1699 9.753299713134766\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print(f\"\\nEpoch: {epoch}\")\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        center, context_negative, label = batch\n",
    "        v = net[0](center.to(torch.int64))\n",
    "        u = net[1](context_negative.to(torch.int64))\n",
    "        pred = torch.tensordot(v, torch.transpose(u, 1, 2))\n",
    "        l = loss(m(pred), label.to(torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(epoch, i, float(l))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation of get_similar_tokens using d2l.ai and PyTorch's CosineSimilarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "cosine sim=0.165: cigarette\n",
      "cosine sim=0.162: british\n",
      "cosine sim=0.143: old\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]  # Access with index == matmul\n",
    "\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos_similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cos = cos_similarity(W, x)\n",
    "\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "Similarity for 'chip':\n",
      "cosine sim=0.165: cigarette\n",
      "cosine sim=0.162: british\n",
      "cosine sim=0.143: old\n",
      "\n",
      "Similarity for 'hardware':\n",
      "cosine sim=0.177: unit\n",
      "cosine sim=0.171: were\n",
      "cosine sim=0.171: <unk>\n",
      "\n",
      "Similarity for 'semiconductor':\n",
      "cosine sim=0.340: the\n",
      "cosine sim=0.312: N\n",
      "cosine sim=0.163: problem\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "words_for_similarity = ['chip', 'hardware', 'semiconductor']\n",
    "\n",
    "for word in words_for_similarity:\n",
    "    print(f\"\\nSimilarity for '{word}':\")\n",
    "    get_similar_tokens(word, 3, net[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}