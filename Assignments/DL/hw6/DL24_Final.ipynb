{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 6 of Deep Learning CS6073\n",
    "    By Akhil Kanna Devarashetti\n",
    "\n",
    "#### Question:\n",
    "\n",
    "    Write a Pytorch version of the Word2vec/skip-gram displayed in Chapter 14 of d2l.  \n",
    "    In particular, make DL24.py error free \n",
    "    Implement get_similar_tokens as an application of the word embedding model \n",
    "    (14.4.3 of d2l and also the last slide in lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 49 14.030824661254883\n",
      "0 99 14.664448738098145\n",
      "0 149 13.80379867553711\n",
      "0 199 11.802692413330078\n",
      "0 249 14.349369049072266\n",
      "0 299 13.037077903747559\n",
      "0 349 13.409360885620117\n",
      "0 399 14.96679401397705\n",
      "0 449 13.498037338256836\n",
      "0 499 12.379167556762695\n",
      "0 549 13.271138191223145\n",
      "0 599 13.458529472351074\n",
      "0 649 13.332769393920898\n",
      "1 49 13.048604011535645\n",
      "1 99 13.343057632446289\n",
      "1 149 13.318685531616211\n",
      "1 199 13.19548511505127\n",
      "1 249 13.479415893554688\n",
      "1 299 13.454062461853027\n",
      "1 349 13.143341064453125\n",
      "1 399 13.37532901763916\n",
      "1 449 12.925860404968262\n",
      "1 499 13.412955284118652\n",
      "1 549 11.702332496643066\n",
      "1 599 13.012636184692383\n",
      "1 649 12.360490798950195\n",
      "2 49 14.150681495666504\n",
      "2 99 11.728529930114746\n",
      "2 149 12.387529373168945\n",
      "2 199 12.636744499206543\n",
      "2 249 14.112762451171875\n",
      "2 299 11.70865535736084\n",
      "2 349 12.183820724487305\n",
      "2 399 12.128985404968262\n",
      "2 449 13.24563217163086\n",
      "2 499 13.121143341064453\n",
      "2 549 11.918919563293457\n",
      "2 599 12.812511444091797\n",
      "2 649 12.174520492553711\n",
      "3 49 13.352450370788574\n",
      "3 99 12.583630561828613\n",
      "3 149 12.095447540283203\n",
      "3 199 13.292035102844238\n",
      "3 249 12.302000999450684\n",
      "3 299 12.239603042602539\n",
      "3 349 13.468535423278809\n",
      "3 399 12.62586784362793\n",
      "3 449 12.353283882141113\n",
      "3 499 12.692530632019043\n",
      "3 549 11.69208812713623\n",
      "3 599 13.18570327758789\n",
      "3 649 11.831883430480957\n",
      "4 49 12.357893943786621\n",
      "4 99 12.556646347045898\n",
      "4 149 12.1299409866333\n",
      "4 199 12.141545295715332\n",
      "4 249 11.763612747192383\n",
      "4 299 11.83621883392334\n",
      "4 349 12.870645523071289\n",
      "4 399 12.683768272399902\n",
      "4 449 12.364066123962402\n",
      "4 499 11.581710815429688\n",
      "4 549 11.727137565612793\n",
      "4 599 12.253012657165527\n",
      "4 649 11.452564239501953\n"
     ]
    }
   ],
   "source": [
    "# DL24.py CS5173/6073 2020 cheng\n",
    "# making centers, contexts, and negatives for PennTreebank data\n",
    "# building vocabulary, performing subsampling and negative sampling\n",
    "# Skip-gram word embedding as a translation from MXNet to Pytorch of d2l chapter 14\n",
    "# Usage: python DL24.py\n",
    "\n",
    "import zipfile\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "f = zipfile.ZipFile('data/ptb.zip', 'r')\n",
    "raw_text = f.read('ptb/ptb.train.txt').decode(\"utf-8\")\n",
    "sentences = [line.split() for line in raw_text.split('\\n')]\n",
    "tokens = [tk for line in sentences for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "uniq_tokens = [token for token, freq in list(counter.items()) if counter[token] >= 10]\n",
    "idx_to_token, token_to_idx = [], dict()\n",
    "for token in uniq_tokens:\n",
    "    idx_to_token.append(token)\n",
    "    token_to_idx[token] = len(idx_to_token) - 1\n",
    "s = [[idx_to_token[token_to_idx.get(tk, 0)] for tk in line] for line in sentences]\n",
    "tokens = [tk for line in s for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "num_tokens = sum(counter.values())\n",
    "subsampled = [[tk for tk in line if random.uniform(0, 1) < math.sqrt(1e-4 / counter[tk] * num_tokens)] for line in s]\n",
    "corpus = [[token_to_idx.get(tk) for tk in line] for line in subsampled]\n",
    "tokens = [tk for line in corpus for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "sampling_weights = [counter[i]**0.75 for i in range(len(counter))]\n",
    "population = list(range(len(sampling_weights)))\n",
    "candidates = random.choices(population, sampling_weights, k=10000)\n",
    "max_window_size = 5\n",
    "K = 5\n",
    "j = 0\n",
    "data = []\n",
    "maxLen = 0\n",
    "for line in corpus:\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    for i in range(len(line)):\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        indices = list(range(max(0, i - window_size), min(len(line), i + 1 + window_size)))\n",
    "        indices.remove(i)\n",
    "        for idx in indices:\n",
    "            context = [line[idx] for idx in indices]\n",
    "        neg = []\n",
    "        while len(neg) < len(context) * K:\n",
    "            ne = candidates[j]\n",
    "            j += 1\n",
    "            if j >= 10000:\n",
    "                j = 0\n",
    "            if ne not in context:\n",
    "                neg.append(ne)\n",
    "        data.append([line[i], context, neg])\n",
    "\n",
    "max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "centers, contexts_negatives, labels = [], [], []\n",
    "for center, context, negative in data:\n",
    "    cur_len = len(context) + len(negative)\n",
    "    centers += [center]\n",
    "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "\n",
    "class PTBdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(PTBdataset).__init__()\n",
    "        self.centers = np.array(centers).reshape(-1, 1)\n",
    "        self.contexts_negatives = np.array(contexts_negatives)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centers[idx], self.contexts_negatives[idx], self.labels[idx]\n",
    "\n",
    "pdata = PTBdataset()\n",
    "data_iter = torch.utils.data.DataLoader(pdata, batch_size=512, shuffle=True)\n",
    "\n",
    "vocab_size = len(idx_to_token)\n",
    "embed_size = 100\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embed_size),\n",
    "    nn.Embedding(vocab_size, embed_size))\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), 0.01)\n",
    "m = nn.Sigmoid()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        center, context_negative, label = batch\n",
    "        v = net[0](center.to(torch.int64))\n",
    "        u = net[1](context_negative.to(torch.int64))\n",
    "        pred = torch.tensordot(v, torch.transpose(u, 1, 2))\n",
    "        l = loss(m(pred), label.to(torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(epoch, i, float(l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementation of get_similar_tokens using d2l.ai and PyTorch's CosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.344: anniversary\n",
      "cosine sim=0.343: public\n",
      "cosine sim=0.329: scores\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    x = x.reshape([1] + list(x.shape))\n",
    "\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    cos = cos_similarity(W, x)\n",
    "\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity for 'five-cent':\n",
      "cosine sim=0.376: hard\n",
      "cosine sim=0.335: trump\n",
      "cosine sim=0.325: antar\n",
      "\n",
      "Similarity for 'hardware':\n",
      "cosine sim=0.341: mix\n",
      "cosine sim=0.338: choice\n",
      "cosine sim=0.317: fallen\n",
      "\n",
      "Similarity for 'semiconductor':\n",
      "cosine sim=0.438: senior\n",
      "cosine sim=0.369: television\n",
      "cosine sim=0.356: everyone\n"
     ]
    }
   ],
   "source": [
    "words_for_similarity = ['five-cent', 'hardware', 'semiconductor']\n",
    "\n",
    "for word in words_for_similarity:\n",
    "    print(f\"\\nSimilarity for '{word}':\")\n",
    "    get_similar_tokens(word, 3, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results for similarity aren't as great as shown in the textbook.\n",
    "This might be because of limited training or/and lack of richer dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
