{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment 6 of Deep Learning CS6073\n",
    "    By Akhil Kanna Devarashetti\n",
    "\n",
    "#### Question:\n",
    "\n",
    "    Write a Pytorch version of the Word2vec/skip-gram displayed in Chapter 14 of d2l.  \n",
    "    In particular, make DL24.py error free \n",
    "    Implement get_similar_tokens as an application of the word embedding model \n",
    "    (14.4.3 of d2l and also the last slide in lecture)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# DL24.py CS5173/6073 2020 cheng\n",
    "# making centers, contexts, and negatives for PennTreebank data\n",
    "# building vocabulary, performing subsampling and negative sampling\n",
    "# Skip-gram word embedding as a translation from MXNet to Pytorch of d2l chapter 14\n",
    "# Usage: python DL24.py\n",
    "\n",
    "import zipfile\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile('data/ptb.zip', 'r')\n",
    "raw_text = f.read('ptb/ptb.train.txt').decode(\"utf-8\")\n",
    "sentences = [line.split() for line in raw_text.split('\\n')]\n",
    "tokens = [tk for line in sentences for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "uniq_tokens = [token for token, freq in list(counter.items()) if counter[token] >= 10]\n",
    "idx_to_token, token_to_idx = [], dict()\n",
    "for token in uniq_tokens:\n",
    "    idx_to_token.append(token)\n",
    "    token_to_idx[token] = len(idx_to_token) - 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Replacing the tokens that are < 10 in frequency with token_to_idx[0] token.\n",
    "s = [[idx_to_token[token_to_idx.get(tk, 0)] for tk in line] for line in sentences]\n",
    "tokens = [tk for line in s for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "num_tokens = sum(counter.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Sample the tokens which are rare.\n",
    "order_of_magnitude = round(math.log10(num_tokens))\n",
    "inverse_frequency = num_tokens / (10 ** order_of_magnitude)\n",
    "\n",
    "subsampled = [[tk for tk in line if random.uniform(0, 1) < math.sqrt(inverse_frequency / counter[tk] * num_tokens)] for line in s]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "corpus = [[token_to_idx.get(tk) for tk in line] for line in subsampled]\n",
    "\n",
    "# corpus ~= [[1, 3, 4, 5], [43, 21, 44, 45]] <- indices of words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tokens = [tk for line in corpus for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "sampling_weights = [counter[i]**0.75 for i in range(len(counter))]\n",
    "population = list(range(len(sampling_weights)))\n",
    "candidates = random.choices(population, sampling_weights, k=(10 ** order_of_magnitude))\n",
    "# candidates = sampled tokens which occur rarely."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The error lies in this loop!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "window_range = 2\n",
    "max_window_size = 5\n",
    "K = 5\n",
    "j = 0\n",
    "data = {}\n",
    "maxLen = 0\n",
    "for line in corpus:\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    for i in range(len(line)):  # i is the center word\n",
    "        # Find a list of context \n",
    "        context, old_negs = data.get(line[i], (set(), set()))\n",
    "        for w in range(-window_range, window_range + 1):\n",
    "            context_word_pos = i + w\n",
    "            if context_word_pos < 0 or context_word_pos >= len(line) or i == context_word_pos or line[i] == line[context_word_pos]:\n",
    "                continue\n",
    "            context_word_idx = line[context_word_pos]\n",
    "            context.add(context_word_idx)\n",
    "            \n",
    "        if len(context) == 0:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for line in corpus:\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    for i in range(len(line)):  # i is the center word\n",
    "        \n",
    "        context, neg = data.get(line[i], (set(), set()))\n",
    "\n",
    "        while len(neg) < len(context) * K:\n",
    "            ne = candidates[j]\n",
    "            j += 1\n",
    "            if j >= (10 ** order_of_magnitude):\n",
    "                j = 0\n",
    "            if ne not in context:\n",
    "                neg.add(ne)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "centers, contexts_negatives, labels = [], [], []\n",
    "for center, context, negative in data:\n",
    "    cur_len = len(context) + len(negative)\n",
    "    centers += [center]\n",
    "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class PTBdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(PTBdataset).__init__()\n",
    "        self.centers = np.array(centers).reshape(-1, 1)\n",
    "        self.contexts_negatives = np.array(contexts_negatives)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centers[idx], self.contexts_negatives[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "pdata = PTBdataset()\n",
    "data_iter = torch.utils.data.DataLoader(pdata, batch_size=512, shuffle=True)\n",
    "\n",
    "vocab_size = len(idx_to_token)\n",
    "embed_size = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embed_size),\n",
    "    nn.Embedding(vocab_size, embed_size))\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), 0.01)\n",
    "m = nn.Sigmoid()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "Epoch: 0\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-aae87fbc42b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_negative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_negative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-aae87fbc42b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_negative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_negative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_36_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_36_64.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_36_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_36_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_36_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_36_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_36_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_36_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\PyCharm 2019.3.2\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py\u001b[0m in \u001b[0;36mstop\u001b[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuspend_jupyter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_debugger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_cmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m             \u001b[0mmain_debugger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\PyCharm 2019.3.2\\plugins\\python\\helpers\\pydev\\pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_wait_suspend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\PyCharm 2019.3.2\\plugins\\python\\helpers\\pydev\\pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print(f\"\\nEpoch: {epoch}\")\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        center, context_negative, label = batch\n",
    "        v = net[0](center.to(torch.int64))\n",
    "        u = net[1](context_negative.to(torch.int64))\n",
    "        pred = torch.tensordot(v, torch.transpose(u, 1, 2))\n",
    "        l = loss(m(pred), label.to(torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(epoch, i, float(l))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation of get_similar_tokens using d2l.ai and PyTorch's CosineSimilarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]  # Access with index == matmul\n",
    "\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos_similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cos = cos_similarity(W, x)\n",
    "\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "Similarity for 'chip':\n",
      "cosine sim=0.261: to\n",
      "cosine sim=0.201: among\n",
      "cosine sim=0.180: nov.\n",
      "\n",
      "Similarity for 'hardware':\n",
      "cosine sim=0.250: gold\n",
      "cosine sim=0.243: to\n",
      "cosine sim=0.188: cancer\n",
      "\n",
      "Similarity for 'semiconductor':\n",
      "cosine sim=0.287: although\n",
      "cosine sim=0.195: ago\n",
      "cosine sim=0.184: journal\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "words_for_similarity = ['chip', 'hardware', 'semiconductor']\n",
    "\n",
    "for word in words_for_similarity:\n",
    "    print(f\"\\nSimilarity for '{word}':\")\n",
    "    get_similar_tokens(word, 3, net[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results for similarity aren't as great as shown in the textbook.\n",
    "This might be because of limited training or/and lack of richer dataset.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}