{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 6\n",
    "    Akhil Kanna Devarashetti\n",
    "\n",
    "Question:\n",
    "\n",
    "    Write a Pytorch version of the Word2vec/skip-gram displayed in Chapter 14 of d2l.  \n",
    "    In particular, make DL24.py error free \n",
    "    Implement get_similar_tokens as an application of the word embedding model \n",
    "    (14.4.3 of d2l and also the last slide in lecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "raw_text = open('ptb/ptb.train.txt', \"r\").read()\n",
    "#raw_text = raw_text[:600]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Get the sentences and tokens (words)\n",
    "sentences = [line.split() for line in raw_text.split('\\n')]\n",
    "tokens = [tk for line in sentences for tk in line]\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "# Get unique tokens (words) with count > 10\n",
    "counter = collections.Counter(tokens)\n",
    "uniq_tokens = [token for token, freq in list(counter.items()) if counter[token] >= 0]  # Make counter 10\n",
    "\n",
    "# Create hash map of the unique words and indices\n",
    "idx_to_token, token_to_idx = uniq_tokens, dict()\n",
    "for i in range(len(uniq_tokens)):\n",
    "    token_to_idx[uniq_tokens[i]] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "subsampled = []\n",
    "\n",
    "for line in sentences:\n",
    "    sub_sampled_line = []\n",
    "    for token in line:\n",
    "        random_number = random.uniform(0, 1)\n",
    "\n",
    "        order_of_magnitude = round(math.log10(num_tokens))\n",
    "        inverse_frequency = num_tokens / (10 ** order_of_magnitude) / counter[token]\n",
    "        \n",
    "        if random_number < inverse_frequency:\n",
    "            sub_sampled_line.append(token)\n",
    "    subsampled.append(sub_sampled_line)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "corpus = [[token_to_idx.get(tk) for tk in line] for line in subsampled]\n",
    "tokens = [tk for line in corpus for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "sampling_weights = [counter[i]**0.75 for i in range(len(counter))]\n",
    "population = list(range(len(sampling_weights)))\n",
    "candidates = random.choices(population, sampling_weights, k=(10**order_of_magnitude))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "max_window_size = 5\n",
    "K = 5\n",
    "j = 0\n",
    "data = []\n",
    "maxLen = 0\n",
    "for line in corpus:\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    for i in range(len(line)):\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        indices = list(range(max(0, i - window_size), min(len(line), i + 1 + window_size)))\n",
    "        indices.remove(i)\n",
    "        for idx in indices:\n",
    "            context = [line[idx] for idx in indices]\n",
    "        neg = []\n",
    "        while len(neg) < len(context) * K:\n",
    "            ne = candidates[j]\n",
    "            j += 1\n",
    "            if j >= 10**order_of_magnitude:\n",
    "                j = 0\n",
    "            if ne not in context:\n",
    "                neg.append(ne)\n",
    "        data.append([line[i], context, neg])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "centers, contexts_negatives, labels = [], [], []\n",
    "for center, context, negative in data:\n",
    "    cur_len = len(context) + len(negative)\n",
    "    centers += [center]\n",
    "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class PTBdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(PTBdataset).__init__()\n",
    "        self.centers = np.array(centers).reshape(-1, 1)\n",
    "        self.contexts_negatives = np.array(contexts_negatives)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centers[idx], self.contexts_negatives[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "pdata = PTBdataset()\n",
    "data_iter = torch.utils.data.DataLoader(pdata, batch_size=512, shuffle=True)\n",
    "\n",
    "vocab_size = len(idx_to_token)\n",
    "embed_size = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embed_size),\n",
    "    nn.Embedding(vocab_size, embed_size))\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), 0.01)\n",
    "m = nn.Sigmoid()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "49 0 13.973994255065918\n",
      "49 1 13.450678825378418\n",
      "49 2 13.671319007873535\n",
      "49 3 13.562156677246094\n",
      "99 0 13.296063423156738\n",
      "99 1 13.219744682312012\n",
      "99 2 13.891067504882812\n",
      "99 3 13.918510437011719\n",
      "149 0 13.91940689086914\n",
      "149 1 12.823521614074707\n",
      "149 2 13.389513969421387\n",
      "149 3 14.116114616394043\n",
      "199 0 13.289380073547363\n",
      "199 1 14.042492866516113\n",
      "199 2 13.431744575500488\n",
      "199 3 13.153682708740234\n",
      "249 0 13.435328483581543\n",
      "249 1 13.22641658782959\n",
      "249 2 14.033463478088379\n",
      "249 3 13.312944412231445\n",
      "299 0 12.956456184387207\n",
      "299 1 13.978198051452637\n",
      "299 2 12.850151062011719\n",
      "299 3 14.403619766235352\n",
      "349 0 13.297236442565918\n",
      "349 1 13.278010368347168\n",
      "349 2 14.101157188415527\n",
      "349 3 13.244428634643555\n",
      "399 0 13.440058708190918\n",
      "399 1 13.90501880645752\n",
      "399 2 13.10141658782959\n",
      "399 3 13.590893745422363\n",
      "449 0 13.274828910827637\n",
      "449 1 14.13455581665039\n",
      "449 2 13.366667747497559\n",
      "449 3 13.13536262512207\n",
      "499 0 13.166953086853027\n",
      "499 1 13.699658393859863\n",
      "499 2 13.853084564208984\n",
      "499 3 13.196040153503418\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        center, context_negative, label = batch\n",
    "        v = net[0](center.to(torch.int64))\n",
    "        u = net[1](context_negative.to(torch.int64))\n",
    "        pred = torch.tensordot(v, torch.transpose(u, 1, 2))\n",
    "        l = loss(m(pred), label.to(torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(epoch, i, float(l))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "cosine sim=0.482: syndrome\n",
      "cosine sim=0.395: budgetary\n",
      "cosine sim=0.330: vision\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Derived from the code in the link: https://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html#finding-synonyms\n",
    "\n",
    "def knn(W, x, k):\n",
    "    cos = torch.matmul(W, x.reshape(-1,)) / (\n",
    "        (torch.sum(W * W, dim=1) + 1e-9).sqrt() * torch.sum(x * x).sqrt())\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    topk = topk.cpu().numpy()\n",
    "    return topk, [cos[i].item() for i in topk]\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    topk, cos = knn(W, x, k+1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):\n",
    "        print('cosine sim=%.3f: %s' % (c, (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('group', 3, net[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "cos_function = torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n",
    "\n",
    "def knn(W, x, k):\n",
    "    print(f\"W shape = {W.shape}\")\n",
    "    print(f\"x shape = {x.shape}\")\n",
    "    cos = cos_function(W, x.reshape(-1,))\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    topk = topk.cpu().numpy()\n",
    "    return topk, [cos[i].item() for i in topk]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words_for_similarity = ['used', 'this', 'director', 'chairman', 'is', 'was']\n",
    "\n",
    "for word in words_for_similarity:\n",
    "    print(f\"\\nSimilarity for '{word}':\")\n",
    "    get_similar_tokens(word, 3, net[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}