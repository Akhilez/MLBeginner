{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 6\n",
    "    Akhil Kanna Devarashetti\n",
    "\n",
    "Question:\n",
    "\n",
    "    Write a Pytorch version of the Word2vec/skip-gram displayed in Chapter 14 of d2l.  \n",
    "    In particular, make DL24.py error free \n",
    "    Implement get_similar_tokens as an application of the word embedding model \n",
    "    (14.4.3 of d2l and also the last slide in lecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "raw_text = open('ptb/ptb.train.txt', \"r\").read()\n",
    "#raw_text = raw_text[:600]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Get the sentences and tokens (words)\n",
    "sentences = [line.split() for line in raw_text.split('\\n')]\n",
    "tokens = [tk for line in sentences for tk in line]\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "# Get unique tokens (words) with count > 10\n",
    "counter = collections.Counter(tokens)\n",
    "uniq_tokens = [token for token, freq in list(counter.items()) if counter[token] >= 0]  # Make counter 10\n",
    "\n",
    "# Create hash map of the unique words and indices\n",
    "idx_to_token, token_to_idx = uniq_tokens, dict()\n",
    "for i in range(len(uniq_tokens)):\n",
    "    token_to_idx[uniq_tokens[i]] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "subsampled = []\n",
    "\n",
    "for line in sentences:\n",
    "    sub_sampled_line = []\n",
    "    for token in line:\n",
    "        random_number = random.uniform(0, 1)\n",
    "\n",
    "        order_of_magnitude = round(math.log10(num_tokens))\n",
    "        inverse_frequency = num_tokens / (10 ** order_of_magnitude) / counter[token]\n",
    "        \n",
    "        if random_number < inverse_frequency:\n",
    "            sub_sampled_line.append(token)\n",
    "    subsampled.append(sub_sampled_line)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "corpus = [[token_to_idx.get(tk) for tk in line] for line in subsampled]\n",
    "tokens = [tk for line in corpus for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "sampling_weights = [counter[i]**0.75 for i in range(len(counter))]\n",
    "population = list(range(len(sampling_weights)))\n",
    "candidates = random.choices(population, sampling_weights, k=(10**order_of_magnitude))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "max_window_size = 5\n",
    "K = 5\n",
    "j = 0\n",
    "data = []\n",
    "maxLen = 0\n",
    "for line in corpus:\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    for i in range(len(line)):\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        indices = list(range(max(0, i - window_size), min(len(line), i + 1 + window_size)))\n",
    "        indices.remove(i)\n",
    "        for idx in indices:\n",
    "            context = [line[idx] for idx in indices]\n",
    "        neg = []\n",
    "        while len(neg) < len(context) * K:\n",
    "            ne = candidates[j]\n",
    "            j += 1\n",
    "            if j >= 10**order_of_magnitude:\n",
    "                j = 0\n",
    "            if ne not in context:\n",
    "                neg.append(ne)\n",
    "        data.append([line[i], context, neg])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "centers, contexts_negatives, labels = [], [], []\n",
    "for center, context, negative in data:\n",
    "    cur_len = len(context) + len(negative)\n",
    "    centers += [center]\n",
    "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class PTBdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(PTBdataset).__init__()\n",
    "        self.centers = np.array(centers).reshape(-1, 1)\n",
    "        self.contexts_negatives = np.array(contexts_negatives)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centers[idx], self.contexts_negatives[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "pdata = PTBdataset()\n",
    "data_iter = torch.utils.data.DataLoader(pdata, batch_size=512, shuffle=True)\n",
    "\n",
    "vocab_size = len(idx_to_token)\n",
    "embed_size = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embed_size),\n",
    "    nn.Embedding(vocab_size, embed_size))\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), 0.01)\n",
    "m = nn.Sigmoid()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "49 0 13.14472770690918\n",
      "49 1 12.709712982177734\n",
      "49 2 12.087530136108398\n",
      "49 3 12.938410758972168\n",
      "49 4 14.469643592834473\n",
      "99 0 12.94942569732666\n",
      "99 1 12.168729782104492\n",
      "99 2 13.257671356201172\n",
      "99 3 12.329767227172852\n",
      "99 4 10.23443603515625\n",
      "149 0 11.750609397888184\n",
      "149 1 13.283474922180176\n",
      "149 2 12.15265941619873\n",
      "149 3 12.907034873962402\n",
      "149 4 12.432740211486816\n",
      "199 0 12.516803741455078\n",
      "199 1 12.042741775512695\n",
      "199 2 12.475598335266113\n",
      "199 3 12.495317459106445\n",
      "199 4 14.040875434875488\n",
      "249 0 13.076385498046875\n",
      "249 1 11.342844009399414\n",
      "249 2 11.80914306640625\n",
      "249 3 13.2349853515625\n",
      "249 4 12.00381088256836\n",
      "299 0 12.395389556884766\n",
      "299 1 11.479875564575195\n",
      "299 2 12.330608367919922\n",
      "299 3 13.210195541381836\n",
      "299 4 11.652397155761719\n",
      "349 0 12.239710807800293\n",
      "349 1 11.882060050964355\n",
      "349 2 12.138818740844727\n",
      "349 3 12.833786964416504\n",
      "349 4 12.242085456848145\n",
      "399 0 12.019271850585938\n",
      "399 1 11.665460586547852\n",
      "399 2 12.980059623718262\n",
      "399 3 12.443427085876465\n",
      "399 4 11.179905891418457\n",
      "449 0 12.609601020812988\n",
      "449 1 12.367752075195312\n",
      "449 2 12.570635795593262\n",
      "449 3 11.448232650756836\n",
      "449 4 11.22389030456543\n",
      "499 0 11.05381965637207\n",
      "499 1 12.23745346069336\n",
      "499 2 12.723772048950195\n",
      "499 3 12.6740083694458\n",
      "499 4 14.057927131652832\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        center, context_negative, label = batch\n",
    "        v = net[0](center.to(torch.int64))\n",
    "        u = net[1](context_negative.to(torch.int64))\n",
    "        pred = torch.tensordot(v, torch.transpose(u, 1, 2))\n",
    "        l = loss(m(pred), label.to(torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(epoch, i, float(l))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "cosine sim=0.414: grenfell\n",
      "cosine sim=0.361: depressed\n",
      "cosine sim=0.349: complaining\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Derived from the code in the link: https://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html#finding-synonyms\n",
    "\n",
    "def knn(W, x, k):\n",
    "    cos = torch.matmul(W, x.reshape(-1,)) / (\n",
    "        (torch.sum(W * W, dim=1) + 1e-9).sqrt() * torch.sum(x * x).sqrt())\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    topk = topk.cpu().numpy()\n",
    "    return topk, [cos[i].item() for i in topk]\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    topk, cos = knn(W, x, k+1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):\n",
    "        print('cosine sim=%.3f: %s' % (c, (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('group', 3, net[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "Similarity for 'chip':\n",
      "cosine sim=0.393: insufficient\n",
      "cosine sim=0.323: h&r\n",
      "cosine sim=0.318: crops\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "words_for_similarity = ['chip']\n",
    "\n",
    "for word in words_for_similarity:\n",
    "    print(f\"\\nSimilarity for '{word}':\")\n",
    "    get_similar_tokens(word, 3, net[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}