{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment 6\n",
    "    Akhil Kanna Devarashetti\n",
    "\n",
    "Question:\n",
    "\n",
    "    Write a Pytorch version of the Word2vec/skip-gram displayed in Chapter 14 of d2l.  \n",
    "    In particular, make DL24.py error free \n",
    "    Implement get_similar_tokens as an application of the word embedding model \n",
    "    (14.4.3 of d2l and also the last slide in lecture)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_text = open('ptb/ptb.train.txt', \"r\").read()\n",
    "#raw_text = raw_text[:600]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Get the sentences and tokens (words)\n",
    "sentences = [line.split() for line in raw_text.split('\\n')]\n",
    "tokens = [tk for line in sentences for tk in line]\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "# Get unique tokens (words) with count > 10\n",
    "counter = collections.Counter(tokens)\n",
    "uniq_tokens = [token for token, freq in list(counter.items()) if counter[token] >= 0]  # Make counter 10\n",
    "\n",
    "# Create hash map of the unique words and indices\n",
    "idx_to_token, token_to_idx = uniq_tokens, dict()\n",
    "for i in range(len(uniq_tokens)):\n",
    "    token_to_idx[uniq_tokens[i]] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subsampled = []\n",
    "\n",
    "for line in sentences:\n",
    "    sub_sampled_line = []\n",
    "    for token in line:\n",
    "        random_number = random.uniform(0, 1)\n",
    "\n",
    "        order_of_magnitude = round(math.log10(num_tokens))\n",
    "        inverse_frequency = num_tokens / (10 ** order_of_magnitude) / counter[token]\n",
    "        \n",
    "        if random_number < inverse_frequency:\n",
    "            sub_sampled_line.append(token)\n",
    "    subsampled.append(sub_sampled_line)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = [[token_to_idx.get(tk) for tk in line] for line in subsampled]\n",
    "tokens = [tk for line in corpus for tk in line]\n",
    "counter = collections.Counter(tokens)\n",
    "sampling_weights = [counter[i]**0.75 for i in range(len(counter))]\n",
    "population = list(range(len(sampling_weights)))\n",
    "candidates = random.choices(population, sampling_weights, k=(10**order_of_magnitude))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_window_size = 5\n",
    "K = 5\n",
    "j = 0\n",
    "data = []\n",
    "maxLen = 0\n",
    "for line in corpus:\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    for i in range(len(line)):\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        indices = list(range(max(0, i - window_size), min(len(line), i + 1 + window_size)))\n",
    "        indices.remove(i)\n",
    "        for idx in indices:\n",
    "            context = [line[idx] for idx in indices]\n",
    "        neg = []\n",
    "        while len(neg) < len(context) * K:\n",
    "            ne = candidates[j]\n",
    "            j += 1\n",
    "            if j >= 10**order_of_magnitude:\n",
    "                j = 0\n",
    "            if ne not in context:\n",
    "                neg.append(ne)\n",
    "        data.append([line[i], context, neg])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "centers, contexts_negatives, labels = [], [], []\n",
    "for center, context, negative in data:\n",
    "    cur_len = len(context) + len(negative)\n",
    "    centers += [center]\n",
    "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PTBdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(PTBdataset).__init__()\n",
    "        self.centers = np.array(centers).reshape(-1, 1)\n",
    "        self.contexts_negatives = np.array(contexts_negatives)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centers[idx], self.contexts_negatives[idx], self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdata = PTBdataset()\n",
    "data_iter = torch.utils.data.DataLoader(pdata, batch_size=512, shuffle=True)\n",
    "\n",
    "vocab_size = len(idx_to_token)\n",
    "embed_size = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embed_size),\n",
    "    nn.Embedding(vocab_size, embed_size))\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), 0.01)\n",
    "m = nn.Sigmoid()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        center, context_negative, label = batch\n",
    "        v = net[0](center.to(torch.int64))\n",
    "        u = net[1](context_negative.to(torch.int64))\n",
    "        pred = torch.tensordot(v, torch.transpose(u, 1, 2))\n",
    "        l = loss(m(pred), label.to(torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(epoch, i, float(l))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Derived from the code in the link: https://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html#finding-synonyms\n",
    "\n",
    "def knn(W, x, k):\n",
    "    cos = torch.matmul(W, x.reshape(-1,)) / (\n",
    "        (torch.sum(W * W, dim=1) + 1e-9).sqrt() * torch.sum(x * x).sqrt())\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    topk = topk.cpu().numpy()\n",
    "    return topk, [cos[i].item() for i in topk]\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    topk, cos = knn(W, x, k+1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):\n",
    "        print('cosine sim=%.3f: %s' % (c, (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('group', 3, net[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words_for_similarity = ['chip']\n",
    "\n",
    "for word in words_for_similarity:\n",
    "    print(f\"\\nSimilarity for '{word}':\")\n",
    "    get_similar_tokens(word, 3, net[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}