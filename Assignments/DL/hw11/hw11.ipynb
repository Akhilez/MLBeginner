{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning CS6073 Assignment 11\n",
    "    Akhil Devarashetti\n",
    "    04/22/2020\n",
    "\n",
    "### Question\n",
    "\n",
    "    DL46.py is an autoencoder to be trained with bdata1.csv.\n",
    "    Run this program\n",
    "    capture the scatter plot of the codes and\n",
    "    display the reconstructed data.\n",
    "    Comment on the appearance of data clustering or \"denoising\" without a training set of \"data without noise\".\n",
    "    \n",
    "   ---\n",
    "    \n",
    "    DL47.py is the same autoencoder.\n",
    "    After training, the encoder parameter is shown and \n",
    "    the user is prompted to select one of the two features learned.\n",
    "    (Select one that has the first five weights and \n",
    "    the last five have opposite signs, if possible.)\n",
    "    Then a gradient ascent will be executed on a random input with the result a \"visualization\" of the feature.\n",
    "    Add code to the program, so for the same trained autoencoder and the same chosen feature,\n",
    "    you also perform gradient descent (maybe on a different random input).\n",
    "    (Instead of \"add\" the gradient to input, you may do \"sub\" for gradient descent.)\n",
    "    \n",
    "   ---\n",
    "    \n",
    "    DL45.py is an incarnation of d2l's GAN with bdata1 as the real_X against fake_X generated by net_G.\n",
    "    A net_D tries to discriminate the two but net_G may see the current output of net_D on its fake data to improve its data generation.\n",
    "    This program is likely to be trapped into mode collapse or other unpleasant result with a few iterations.\n",
    "    Run it and show the best result you can get.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "# DL46.py\n",
    "\n",
    "    DL46.py is an autoencoder to be trained with bdata1.csv.\n",
    "    Run this program\n",
    "    capture the scatter plot of the codes and\n",
    "    display the reconstructed data.\n",
    "    Comment on the appearance of data clustering or \"denoising\" without a training set of \"data without noise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "data3 = torch.tensor(pd.read_csv(\"bdata1.csv\").to_numpy(), dtype=torch.float32)\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model1 = nn.Sequential(\n",
    "\tnn.Linear(10, 2, bias=False),\n",
    "\tnn.Linear(2, 10, bias=False))\n",
    "\n",
    "optimizer = optim.SGD(model1.parameters(), 1e-2, momentum=0.3, nesterov=True)\n",
    "loss_fun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.6618431806564331\n",
      "Epoch 1000, Loss 0.2147756814956665\n",
      "Epoch 2000, Loss 0.1371237337589264\n",
      "Epoch 3000, Loss 0.09738676995038986\n",
      "Epoch 4000, Loss 0.09347891062498093\n",
      "Epoch 5000, Loss 0.0930076539516449\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    p = model1(data3)\n",
    "    loss = loss_fun(p, data3)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\tif epoch == 1 or epoch % 1000 == 0:\n",
    "\t\tprint('Epoch {}, Loss {}'.format(epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9920,  0.3900,  0.4395,  1.0429,  0.6831,  1.8804,  1.5716,  1.3314,\n",
      "          1.3280,  1.0637],\n",
      "        [ 1.1481,  1.1336,  1.4519,  0.8724,  1.4437,  0.3978, -0.1900, -0.0901,\n",
      "         -0.1819,  0.1614]])\n"
     ]
    }
   ],
   "source": [
    "codes = model1[0](data3).data.T\n",
    "print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed: \n",
      "[[1 1 1 1 1 1 0 1 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 0 1 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 1]]\n",
      "Real: \n",
      "[[0 1 1 1 1 1 0 1 0 0]\n",
      " [1 0 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 0 0 1 1 0 1 0 0]\n",
      " [1 1 1 1 1 0 0 0 1 0]\n",
      " [1 0 0 0 1 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 0]\n",
      " [0 0 0 0 0 1 1 1 0 1]\n",
      " [0 0 0 0 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (3.5, 2.5)\n",
    "plt.scatter(codes[0], codes[1])\n",
    "plt.show()\n",
    "\n",
    "reconstructed = model1(data3).gt(0.5).int()\n",
    "\n",
    "print(\"Reconstructed: \")\n",
    "print(reconstructed.numpy().astype(np.int32))\n",
    "\n",
    "print(\"Real: \")\n",
    "print(data3.numpy().astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "## Comment\n",
    "\n",
    "The autoencoder in general is recognizing two kinds of data patterns.\n",
    "It is failing to capture the fine details in the data, but doing a good job at generalizing the data pattern.\n",
    "The hidden layer that encodes the input must encode all the information required for reconstruction into two floating point numbers. Then the decoder would use these two floats and recreate the input.\n",
    "However, the autoencoder finds it difficult to capture and recreate the \"noisy\" elements (the fine details) of the input with just two units in the hidden layer.\n",
    "\n",
    "This results in a the decoder generating a more generalized data rather than a very specific data point.\n",
    "Thats the reason that I can infer for the \"denoising\" of the data after passing it though the autoencoder.\n",
    "\n",
    "Comment on the scatter plot:\n",
    "It looks like the relationship between the weights of the two neurons is that they are inversely proportional. When the weight for nuron 1 on input 1 (code[0][0]) is higher, the weight for the neuron 2 on the same input (code[1][0]) is low. This is seen for all the weights of the two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# DL47.py\n",
    "\n",
    "    DL47.py is the same autoencoder.\n",
    "    After training, the encoder parameter is shown and \n",
    "    the user is prompted to select one of the two features learned.\n",
    "    (Select one that has the first five weights and the last five have opposite signs, if possible.)\n",
    "    Then a gradient ascent will be executed on a random input with the result a \"visualization\" of the feature.\n",
    "    Add code to the program, so for the same trained autoencoder and the same chosen feature,\n",
    "    you also perform gradient descent (maybe on a different random input).\n",
    "    (Instead of \"add\" the gradient to input, you may do \"sub\" for gradient descent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1321,  0.0495,  0.0101,  0.0711,  0.1767,  0.1990,  0.4031,  0.4857,\n",
       "           0.2436,  0.2402],\n",
       "         [ 0.2701,  0.3183,  0.4030,  0.1427,  0.3178, -0.0162, -0.0482, -0.0175,\n",
       "          -0.0081, -0.1000]], requires_grad=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model1[0].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the hidden node (0 or 1) whose feature you want to enhance\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('Enter the hidden node (0 or 1) whose feature you want to enhance')\n",
    "input1 = int(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8528, 0.2930, 0.4550, 0.1334, 0.1538, 0.7639, 0.8828, 0.0642, 0.5089,\n",
      "         0.0411]], requires_grad=True)\n",
      "tensor([[1, 0, 0, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data4 = torch.tensor(np.random.random((1, 10)), dtype=torch.float32, requires_grad=True)\n",
    "print(data4)\n",
    "print(data4.gt(0.5).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 1, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 0, 1, 1, 1, 0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 0, 1, 1, 1, 0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 0, 1, 1, 1, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    c = model1[0](data4)\n",
    "    c[0][input1].backward()\n",
    "    data4 = data4.add(data4.grad * 0.5).clone().detach().requires_grad_(True)\n",
    "    print(data4.gt(0.5).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent on random input\n",
    "This will make the input look more like the data that neuron 1 activates the most which is of the type where zeros are on left, and ones on the right `[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2912, 0.7072, 0.0998, 0.1542, 0.4988, 0.7374, 0.8183, 0.3873, 0.7616,\n",
      "         0.1649]], requires_grad=True)\n",
      "tensor([[0, 1, 0, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data5 = torch.tensor(np.random.random((1, 10)), dtype=torch.float32, requires_grad=True)\n",
    "print(data5)\n",
    "print(data5.gt(0.5).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n",
      "tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n",
      "tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n",
      "tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n",
      "tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    c = model1[0](data5)\n",
    "    c[0][input1].backward()\n",
    "    data5 = data5.sub(data5.grad * 0.5).clone().detach().requires_grad_(True)\n",
    "    print(data5.gt(0.5).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "# DL45.py\n",
    "\n",
    "    DL45.py is an incarnation of d2l's GAN with bdata1 as the real_X against fake_X generated by net_G.\n",
    "    A net_D tries to discriminate the two but net_G may see the current output of net_D on its fake data to improve its data generation.\n",
    "    This program is likely to be trapped into mode collapse or other unpleasant result with a few iterations.\n",
    "    Run it and show the best result you can get.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1, 1, 1, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 0, 1, 0, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv(\"bdata1.csv\")\n",
    "data2 = data1.to_numpy()\n",
    "real_X = torch.tensor(data2, dtype=torch.float32)\n",
    "print(real_X.gt(0.5).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size, latent_dim, n = 10, 2, 10\n",
    "net_G = nn.Sequential(nn.Linear(latent_dim, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1637, -1.1303],\n",
      "        [ 0.5227, -0.0152],\n",
      "        [-2.2803, -0.5095],\n",
      "        [-1.7380, -0.1835],\n",
      "        [ 0.8742, -1.5675],\n",
      "        [-1.7220,  2.1002],\n",
      "        [-1.1817, -0.5826],\n",
      "        [-3.5056,  0.1477],\n",
      "        [ 1.3975, -1.3721],\n",
      "        [ 0.3178, -1.4008]])\n",
      "tensor([[0, 0, 0, 1, 0, 1, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 0, 1, 1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "Z = torch.tensor(np.random.normal(0, 1, size=(batch_size, latent_dim)), \n",
    "                 dtype=torch.float32)\n",
    "fake_X = net_G(Z)\n",
    "print(Z)\n",
    "# print(fake_X)\n",
    "print(fake_X.gt(0.5).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net_D = nn.Sequential(\n",
    "    nn.Linear(n, latent_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(latent_dim, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "ones = torch.ones((batch_size,1))\n",
    "zeros = torch.zeros((batch_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr_D, lr_G, num_epochs = 0.05, 0.01, 100\n",
    "trainer_D = optim.Adam(net_D.parameters(), lr_D)\n",
    "trainer_G = optim.Adam(net_G.parameters(), lr_G)\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_D():\n",
    "    \"\"\"Update discriminator\"\"\"\n",
    "    real_Y = net_D(real_X)\n",
    "    fake_X = net_G(Z)\n",
    "    fake_Y = net_D(fake_X.detach())\n",
    "    loss_D = (loss(real_Y, ones) + loss(fake_Y, zeros)) / 2\n",
    "    trainer_D.zero_grad()\n",
    "    loss_D.backward()\n",
    "    trainer_D.step()\n",
    "    return float(loss_D.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_G(): \n",
    "    \"\"\"Update generator\"\"\"\n",
    "    # Recomputing fake_Y is needed since net_D is changed.\n",
    "    fake_X = net_G(Z)\n",
    "    fake_Y = net_D(fake_X)\n",
    "    loss_G = loss(fake_Y, ones)\n",
    "    trainer_G.zero_grad()\n",
    "    loss_G.backward()\n",
    "    trainer_G.step()\n",
    "    return float(loss_G.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000\n",
      "0.6931471824645996 0.6931471824645996\n",
      "new fakes\n",
      "tensor([[1, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], dtype=torch.int32)\n",
      "discriminator's output to new fakes\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "epoch 2000\n",
      "0.6931471824645996 0.6931471824645996\n",
      "new fakes\n",
      "tensor([[0, 1, 0, 1, 0, 1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "discriminator's output to new fakes\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "epoch 3000\n",
      "0.6931471824645996 0.6931471824645996\n",
      "new fakes\n",
      "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "discriminator's output to new fakes\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "epoch 4000\n",
      "0.6931471824645996 0.6931471824645996\n",
      "new fakes\n",
      "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 0, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 1, 1, 0, 0, 1, 0]], dtype=torch.int32)\n",
      "discriminator's output to new fakes\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "epoch 5000\n",
      "0.6931471824645996 0.6931471824645996\n",
      "new fakes\n",
      "tensor([[0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 0, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "discriminator's output to new fakes\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_G = update_G()\n",
    "    loss_D = update_D()\n",
    "    Z = torch.tensor(np.random.normal(0, 1, size=(batch_size, latent_dim)), \n",
    "                 dtype=torch.float32)\n",
    "    if (epoch % 1000 == 0):\n",
    "        print('epoch', epoch)\n",
    "        print(loss_G, loss_D)\n",
    "        new_fake_X = net_G(Z)\n",
    "        print('new fakes')\n",
    "        print(new_fake_X.gt(0.5).int())\n",
    "        print(\"discriminator's output to new fakes\")\n",
    "        print(net_D(new_fake_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake Data: \n",
      "tensor([[1, 1, 1, 0, 1, 1, 1, 0, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 0, 0, 1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "Actual Data:\n",
      "tensor([[0, 1, 1, 1, 1, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 1, 1, 0, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 1, 0, 1, 0, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "Z = torch.tensor(np.random.normal(0, 1, size=(batch_size, latent_dim)), \n",
    "                 dtype=torch.float32)\n",
    "\n",
    "new_fake_X = net_G(Z)\n",
    "print('Fake Data: ')\n",
    "print(new_fake_X.gt(0.5).int())\n",
    "print(\"Actual Data:\")\n",
    "print(real_X.gt(0.5).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment:\n",
    "The GAN doesn't always generate the best data.\n",
    "Infact, the discriminator cannot distinguish between the real or fake data.\n",
    "\n",
    "The above is one of the better generated data.\n",
    "We can see that at index 1 of fake data `[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]`, it is similat the the first half of actual data i.e. more ones on left side, zeros on right side `[1, 1, 1, 1, 1, 0, 0, 0, 1, 0]`.\n",
    "At the last index (9) of the fake data `[0, 1, 0, 0, 0, 1, 1, 1, 1, 1]`, we can see that this data is similar to the second type of real data i.e. more zeros on left, ones on right `[0, 0, 0, 0, 0, 1, 1, 1, 1, 0]`.\n",
    "\n",
    "However, the data at indices 0 `[1, 1, 1, 0, 1, 1, 1, 0, 1, 1]`, 5 `[0, 1, 0, 1, 0, 1, 1, 0, 0, 0]` and 6 `[0, 1, 0, 1, 1, 1, 1, 0, 0, 0]` the data does not look regular, but is similar to the real data at index 3 `[1, 1, 0, 0, 1, 1, 0, 1, 0, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
